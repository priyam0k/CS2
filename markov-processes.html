<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 9 Markov processes | Risk Modelling and Survival Analysis</title>
  <meta name="description" content="A modified version of the book on Risk Modelling and Survival Analysis, with additional notes and examples." />
  <meta name="generator" content="bookdown 0.43 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 9 Markov processes | Risk Modelling and Survival Analysis" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="A modified version of the book on Risk Modelling and Survival Analysis, with additional notes and examples." />
  <meta name="github-repo" content="priyam0k/CS2" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 9 Markov processes | Risk Modelling and Survival Analysis" />
  
  <meta name="twitter:description" content="A modified version of the book on Risk Modelling and Survival Analysis, with additional notes and examples." />
  

<meta name="author" content="Priyam" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  <link rel="shortcut icon" href="img/favicon.ico" type="image/x-icon" />
<link rel="prev" href="markov-chains.html"/>
<link rel="next" href="survival-models.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Risk Modelling and Survival Analysis</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#motivation"><i class="fa fa-check"></i>Motivation</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="r-setup.html"><a href="r-setup.html"><i class="fa fa-check"></i><b>1</b> R Setup</a>
<ul>
<li class="chapter" data-level="1.1" data-path="r-setup.html"><a href="r-setup.html#preparing-your-environment"><i class="fa fa-check"></i><b>1.1</b> Preparing your environment</a></li>
<li class="chapter" data-level="1.2" data-path="r-setup.html"><a href="r-setup.html#basic-interations-with-r"><i class="fa fa-check"></i><b>1.2</b> Basic interations with R</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="loss-distributions.html"><a href="loss-distributions.html"><i class="fa fa-check"></i><b>2</b> Loss distributions</a>
<ul>
<li class="chapter" data-level="" data-path="loss-distributions.html"><a href="loss-distributions.html#objectives-loss-distributions"><i class="fa fa-check"></i>Learning Objectives</a></li>
<li class="chapter" data-level="" data-path="loss-distributions.html"><a href="loss-distributions.html#theory-loss-distributions"><i class="fa fa-check"></i>Theory</a></li>
<li class="chapter" data-level="" data-path="loss-distributions.html"><a href="loss-distributions.html#practice-loss-distributions"><i class="fa fa-check"></i><code>R</code> Practice</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="compound-loss-distributions.html"><a href="compound-loss-distributions.html"><i class="fa fa-check"></i><b>3</b> Compound loss distributions</a>
<ul>
<li class="chapter" data-level="" data-path="compound-loss-distributions.html"><a href="compound-loss-distributions.html#objectives-compound-loss-distributions"><i class="fa fa-check"></i>Learning Objectives</a></li>
<li class="chapter" data-level="" data-path="compound-loss-distributions.html"><a href="compound-loss-distributions.html#theory-compound-loss-distributions"><i class="fa fa-check"></i>Theory</a>
<ul>
<li class="chapter" data-level="3.0.1" data-path="compound-loss-distributions.html"><a href="compound-loss-distributions.html#chapter-compound-loss-distributions"><i class="fa fa-check"></i><b>3.0.1</b> <strong>Chapter: Compound Loss Distributions</strong></a></li>
</ul></li>
<li class="chapter" data-level="" data-path="compound-loss-distributions.html"><a href="compound-loss-distributions.html#practice-compound-loss-distributions"><i class="fa fa-check"></i><code>R</code> Practice</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="copulas.html"><a href="copulas.html"><i class="fa fa-check"></i><b>4</b> Copulas</a>
<ul>
<li class="chapter" data-level="" data-path="copulas.html"><a href="copulas.html#objectives-copulas"><i class="fa fa-check"></i>Learning Objectives</a></li>
<li class="chapter" data-level="" data-path="copulas.html"><a href="copulas.html#theory-copulas"><i class="fa fa-check"></i>Theory</a>
<ul>
<li class="chapter" data-level="4.0.1" data-path="copulas.html"><a href="copulas.html#chapter-copulas-modelling-dependency-structures"><i class="fa fa-check"></i><b>4.0.1</b> <strong>Chapter: Copulas – Modelling Dependency Structures</strong></a></li>
</ul></li>
<li class="chapter" data-level="" data-path="copulas.html"><a href="copulas.html#practice-copulas"><i class="fa fa-check"></i><code>R</code> Practice</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="extreme-value-theory.html"><a href="extreme-value-theory.html"><i class="fa fa-check"></i><b>5</b> Extreme value theory</a>
<ul>
<li class="chapter" data-level="" data-path="extreme-value-theory.html"><a href="extreme-value-theory.html#objectives-evt"><i class="fa fa-check"></i>Learning Objectives</a></li>
<li class="chapter" data-level="" data-path="extreme-value-theory.html"><a href="extreme-value-theory.html#theory-evt"><i class="fa fa-check"></i>Theory</a>
<ul>
<li class="chapter" data-level="5.0.1" data-path="extreme-value-theory.html"><a href="extreme-value-theory.html#chapter-16-extreme-value-theory"><i class="fa fa-check"></i><b>5.0.1</b> Chapter 16: Extreme Value Theory</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="extreme-value-theory.html"><a href="extreme-value-theory.html#practice-evt"><i class="fa fa-check"></i><code>R</code> Practice</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="time-series.html"><a href="time-series.html"><i class="fa fa-check"></i><b>6</b> Time series</a>
<ul>
<li class="chapter" data-level="" data-path="time-series.html"><a href="time-series.html#objectives-time-series"><i class="fa fa-check"></i>Learning Objectives</a></li>
<li class="chapter" data-level="6.1" data-path="time-series.html"><a href="time-series.html#theory-time-series"><i class="fa fa-check"></i><b>6.1</b> Theory</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="time-series.html"><a href="time-series.html#time-series-a-deep-dive-for-cs2-actuarial-professionals"><i class="fa fa-check"></i><b>6.1.1</b> Time Series: A Deep Dive for CS2 Actuarial Professionals</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="time-series.html"><a href="time-series.html#practice-time-series"><i class="fa fa-check"></i><code>R</code> Practice</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="stochastic-processes.html"><a href="stochastic-processes.html"><i class="fa fa-check"></i><b>7</b> Stochastic processes</a>
<ul>
<li class="chapter" data-level="" data-path="stochastic-processes.html"><a href="stochastic-processes.html#objectives-stochastic-processes"><i class="fa fa-check"></i>Learning Objectives</a></li>
<li class="chapter" data-level="" data-path="stochastic-processes.html"><a href="stochastic-processes.html#theory-stochastic-processes"><i class="fa fa-check"></i>Theory</a>
<ul>
<li class="chapter" data-level="7.0.1" data-path="stochastic-processes.html"><a href="stochastic-processes.html#chapter-1-stochastic-processes"><i class="fa fa-check"></i><b>7.0.1</b> Chapter 1: Stochastic Processes</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="stochastic-processes.html"><a href="stochastic-processes.html#practice-stochastic-processes"><i class="fa fa-check"></i><code>R</code> Practice</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="markov-chains.html"><a href="markov-chains.html"><i class="fa fa-check"></i><b>8</b> Markov chains</a>
<ul>
<li class="chapter" data-level="" data-path="markov-chains.html"><a href="markov-chains.html#objectives-markov-chains"><i class="fa fa-check"></i>Learning Objectives</a></li>
<li class="chapter" data-level="" data-path="markov-chains.html"><a href="markov-chains.html#theory-markov-chains"><i class="fa fa-check"></i>Theory</a>
<ul>
<li class="chapter" data-level="8.0.1" data-path="markov-chains.html"><a href="markov-chains.html#chapter-markov-chains-a-comprehensive-overview"><i class="fa fa-check"></i><b>8.0.1</b> Chapter: Markov Chains – A Comprehensive Overview</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="markov-chains.html"><a href="markov-chains.html#practice-markov-chains"><i class="fa fa-check"></i><code>R</code> Practice</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="markov-processes.html"><a href="markov-processes.html"><i class="fa fa-check"></i><b>9</b> Markov processes</a>
<ul>
<li class="chapter" data-level="" data-path="markov-processes.html"><a href="markov-processes.html#objectives-09"><i class="fa fa-check"></i>Learning Objectives</a></li>
<li class="chapter" data-level="" data-path="markov-processes.html"><a href="markov-processes.html#theory-09"><i class="fa fa-check"></i>Theory</a>
<ul>
<li class="chapter" data-level="9.0.1" data-path="markov-processes.html"><a href="markov-processes.html#chapter-markov-processes-syllabus-objectives-3.3.1---3.3.8"><i class="fa fa-check"></i><b>9.0.1</b> <strong>Chapter: Markov Processes (Syllabus Objectives 3.3.1 - 3.3.8)</strong></a></li>
</ul></li>
<li class="chapter" data-level="" data-path="markov-processes.html"><a href="markov-processes.html#practice-09"><i class="fa fa-check"></i><code>R</code> Practice</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="survival-models.html"><a href="survival-models.html"><i class="fa fa-check"></i><b>10</b> Survival models</a></li>
<li class="chapter" data-level="11" data-path="lifetime-distributions.html"><a href="lifetime-distributions.html"><i class="fa fa-check"></i><b>11</b> Lifetime distributions</a></li>
<li class="chapter" data-level="12" data-path="transition-intensities.html"><a href="transition-intensities.html"><i class="fa fa-check"></i><b>12</b> Estimating transition intensities</a></li>
<li class="chapter" data-level="13" data-path="graduation.html"><a href="graduation.html"><i class="fa fa-check"></i><b>13</b> Graduation</a></li>
<li class="chapter" data-level="14" data-path="mortality-projection.html"><a href="mortality-projection.html"><i class="fa fa-check"></i><b>14</b> Mortality projection</a></li>
<li class="chapter" data-level="15" data-path="machine-learning.html"><a href="machine-learning.html"><i class="fa fa-check"></i><b>15</b> Machine learning</a>
<ul>
<li class="chapter" data-level="" data-path="machine-learning.html"><a href="machine-learning.html#objectives-15"><i class="fa fa-check"></i>Learning Objectives</a></li>
<li class="chapter" data-level="" data-path="machine-learning.html"><a href="machine-learning.html#theory-15"><i class="fa fa-check"></i>Theory</a></li>
<li class="chapter" data-level="15.1" data-path="machine-learning.html"><a href="machine-learning.html#machine-learning-topics"><i class="fa fa-check"></i><b>15.1</b> Machine learning topics</a></li>
<li class="chapter" data-level="15.2" data-path="machine-learning.html"><a href="machine-learning.html#machine-learning-from-data"><i class="fa fa-check"></i><b>15.2</b> Machine learning from data</a></li>
<li class="chapter" data-level="15.3" data-path="machine-learning.html"><a href="machine-learning.html#supervised-machine-learning"><i class="fa fa-check"></i><b>15.3</b> Supervised machine learning</a></li>
<li class="chapter" data-level="15.4" data-path="machine-learning.html"><a href="machine-learning.html#unsupervised-machine-learning"><i class="fa fa-check"></i><b>15.4</b> Unsupervised machine learning</a></li>
<li class="chapter" data-level="15.5" data-path="machine-learning.html"><a href="machine-learning.html#penalised-regression"><i class="fa fa-check"></i><b>15.5</b> Penalised regression</a></li>
<li class="chapter" data-level="15.6" data-path="machine-learning.html"><a href="machine-learning.html#decision-trees"><i class="fa fa-check"></i><b>15.6</b> Decision trees</a></li>
<li class="chapter" data-level="15.7" data-path="machine-learning.html"><a href="machine-learning.html#perspectives-of-non-actuarial-professionals"><i class="fa fa-check"></i><b>15.7</b> Perspectives of non-actuarial professionals</a></li>
<li class="chapter" data-level="" data-path="machine-learning.html"><a href="machine-learning.html#practice-15"><i class="fa fa-check"></i><code>R</code> Practice</a></li>
</ul></li>
<li class="divider"></li>
<li>Adapted by <a href="https://github.com/priyam0k">Priyam</a> from the original by <a href="https://github.com/agarbiak" target="blank">Alex Garbiak</a>.</li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Risk Modelling and Survival Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="markov-processes" class="section level1 hasAnchor" number="9">
<h1><span class="header-section-number">Chapter 9</span> Markov processes<a href="markov-processes.html#markov-processes" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="objectives-09" class="section level2 unnumbered hasAnchor">
<h2>Learning Objectives<a href="markov-processes.html#objectives-09" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ol style="list-style-type: decimal">
<li>State the essential features of a Markov process model.</li>
<li>Define a Poisson process, derive the distribution of the number of events in a given time interval, derive the distribution of inter-event times, and apply these results.</li>
<li>Derive the Kolmogorov equations for a Markov process with time independent and time/age dependent transition intensities.</li>
<li>Solve the Kolmogorov equations in simple cases.</li>
<li>State the Kolmogorov equations for a model where the transition intensities depend not only on age/time, but also on the duration of stay in one or more states.</li>
<li>Describe sickness and marriage models in terms of duration dependent Markov processes and describe other simple applications.</li>
<li>Demonstrate how Markov jump processes can be used as a tool for modelling and how they can be simulated.</li>
</ol>
</div>
<div id="theory-09" class="section level2 unnumbered hasAnchor">
<h2>Theory<a href="markov-processes.html#theory-09" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="chapter-markov-processes-syllabus-objectives-3.3.1---3.3.8" class="section level3 hasAnchor" number="9.0.1">
<h3><span class="header-section-number">9.0.1</span> <strong>Chapter: Markov Processes (Syllabus Objectives 3.3.1 - 3.3.8)</strong><a href="markov-processes.html#chapter-markov-processes-syllabus-objectives-3.3.1---3.3.8" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>This section is vital, contributing significantly to the “Stochastic processes” syllabus topic, which holds a substantial 25% weighting in your CS2 assessment. Markov processes, particularly Markov jump processes, are continuous-time models of phenomena that evolve randomly over time, driven by their defining Markov property.</p>
<hr />
<div id="objective-1-state-the-essential-features-of-a-markov-process-model." class="section level4 hasAnchor" number="9.0.1.1">
<h4><span class="header-section-number">9.0.1.1</span> <strong>Objective 1: State the essential features of a Markov process model.</strong><a href="markov-processes.html#objective-1-state-the-essential-features-of-a-markov-process-model." class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>A Markov process, at its heart, is a stochastic process exhibiting a unique and powerful characteristic: the <strong>Markov property</strong>.</p>
<ol style="list-style-type: decimal">
<li><strong>Stochastic Process Foundation</strong>:
<ul>
<li>A stochastic process is fundamentally a model for a time-dependent random phenomenon. It’s a collection of random variables, denoted as <span class="math inline">\(\{X_t : t \in J\}\)</span>, where each <span class="math inline">\(X_t\)</span> models the value of the process at time <span class="math inline">\(t\)</span> within some defined time set <span class="math inline">\(J\)</span>.</li>
<li>Processes can be classified based on their <strong>time set</strong> (<span class="math inline">\(J\)</span>) and <strong>state space</strong> (<span class="math inline">\(S\)</span>):
<ul>
<li><strong>Continuous Time, Discrete State Space</strong>: This is the category for a <strong>Markov jump process</strong>. Examples include the number of claims arriving at an insurance company, an individual’s health status (healthy, sick, dead), or the number of occupied parking spaces.</li>
<li><strong>Discrete Time, Discrete State Space</strong>: This defines a <strong>Markov chain</strong>. Think of no-claims discount levels in motor insurance, where changes occur at fixed yearly intervals.</li>
<li><strong>Discrete Time, Continuous State Space</strong>: This is characteristic of <strong>time series</strong> (e.g., daily stock prices) or general random walks.</li>
<li><strong>Continuous Time, Continuous State Space</strong>: Examples include Brownian motion or diffusion processes, typically covered in CM2.</li>
<li><strong>Mixed Type</strong>: Some processes might transition between these, like pension scheme contributors (continuous time with discrete changes).</li>
</ul></li>
</ul></li>
<li><strong>The Markov Property</strong>:
<ul>
<li>The defining feature: “A major simplification occurs if the future development of a process can be predicted from its present state alone, without any reference to its past history”.</li>
<li>Mathematically, for any times <span class="math inline">\(s_1 &lt; s_2 &lt; \dots &lt; s_n &lt; s &lt; t\)</span> and states <span class="math inline">\(x_1, \dots, x_n, x\)</span>, and any subset <span class="math inline">\(A\)</span> of the state space <span class="math inline">\(S\)</span>:
<span class="math inline">\(P[X_t \in A | X_{s_1}=x_1, \dots, X_{s_n}=x_n, X_s=x] = P[X_t \in A | X_s=x]\)</span>.</li>
<li>In simpler terms, given the current state, additional knowledge of the past is irrelevant for predicting the future.</li>
<li>A significant implication: processes with <strong>independent increments</strong> (where increments over non-overlapping intervals are statistically independent) always possess the Markov property.</li>
</ul></li>
</ol>
<hr />
</div>
<div id="objective-2-define-a-poisson-process-derive-the-distribution-of-the-number-of-events-in-a-given-time-interval-derive-the-distribution-of-inter-event-times-and-apply-these-results." class="section level4 hasAnchor" number="9.0.1.2">
<h4><span class="header-section-number">9.0.1.2</span> <strong>Objective 2: Define a Poisson process, derive the distribution of the number of events in a given time interval, derive the distribution of inter-event times, and apply these results.</strong><a href="markov-processes.html#objective-2-define-a-poisson-process-derive-the-distribution-of-the-number-of-events-in-a-given-time-interval-derive-the-distribution-of-inter-event-times-and-apply-these-results." class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The Poisson process is the simplest, yet incredibly fundamental, example of a time-homogeneous Markov jump process in continuous time.</p>
<ol style="list-style-type: decimal">
<li><strong>Definition of a Poisson Process</strong>:
A continuous-time, integer-valued process <span class="math inline">\(\{N_t : t \ge 0\}\)</span> with rate <span class="math inline">\(\lambda &gt; 0\)</span> can be defined equivalently by any of the following four conditions:
<ul>
<li><strong>(1) Increments and Distribution</strong>: It has stationary, independent increments, and for each <span class="math inline">\(t\)</span>, <span class="math inline">\(N_t\)</span> has a Poisson distribution with parameter <span class="math inline">\(\lambda t\)</span>.</li>
<li><strong>(2) Short-Time Transition Probabilities (Markov Jump Process perspective)</strong>: It is a Markov jump process with independent increments and transition probabilities over a short time period <span class="math inline">\(h\)</span> given by:
<ul>
<li><span class="math inline">\(P[N_{t+h} - N_t = 1 | \mathcal{F}_t] = \lambda h + o(h)\)</span>.</li>
<li><span class="math inline">\(P[N_{t+h} - N_t = 0 | \mathcal{F}_t] = 1 - \lambda h + o(h)\)</span>.</li>
<li><span class="math inline">\(P[N_{t+h} - N_t \ge 2 | \mathcal{F}_t] = o(h)\)</span>.
(Here, <span class="math inline">\(o(h)\)</span> represents a function such that <span class="math inline">\(\lim_{h \to 0} o(h)/h = 0\)</span>).</li>
</ul></li>
<li><strong>(3) Holding Times (Inter-event times)</strong>: The holding times (or inter-event times) <span class="math inline">\(T_0, T_1, T_2, \dots\)</span> are independent Exponential random variables with parameter <span class="math inline">\(\lambda\)</span>.</li>
<li><strong>(4) Transition Rates (<span class="math inline">\(\mu_{ij}\)</span>)</strong>: It is a Markov jump process with independent increments and transition rates given by <span class="math inline">\(\mu_{i,i+1} = \lambda\)</span> and <span class="math inline">\(\mu_{ii} = -\lambda\)</span>, with other rates being 0. This implies jumps only occur to the next integer state.</li>
</ul></li>
<li><strong>Distribution of the Number of Events in a Given Time Interval</strong>:
<ul>
<li>From definition (1) above, the number of events <span class="math inline">\(N_t\)</span> in a time interval <span class="math inline">\([0, t]\)</span> (starting from <span class="math inline">\(N_0=0\)</span>) follows a <strong>Poisson distribution with parameter <span class="math inline">\(\lambda t\)</span></strong>.</li>
<li>More generally, for any <span class="math inline">\(s &lt; t\)</span>, the increment <span class="math inline">\(N_t - N_s\)</span> is Poisson distributed with mean <span class="math inline">\(\lambda(t-s)\)</span>, and it is independent of anything that occurred before time <span class="math inline">\(s\)</span>.</li>
</ul></li>
<li><strong>Distribution of Inter-event Times</strong>:
<ul>
<li>As per definition (3), the successive inter-event times <span class="math inline">\(T_0, T_1, T_2, \dots\)</span> are independent and identically distributed <strong>Exponential random variables with parameter <span class="math inline">\(\lambda\)</span></strong>. This is a direct consequence of the memoryless property of the Exponential distribution.</li>
</ul></li>
<li><strong>Applications and Properties</strong>:
<ul>
<li><strong>Applications</strong>: Poisson processes are fundamental for counting the cumulative number of occurrences of events over time, such as motor insurance claims, customer arrivals at a service point, or occurrences of claims events (e.g., accidents, fires, thefts).</li>
<li><strong>Non-Stationarity</strong>: A Poisson process is <em>not</em> weakly stationary because its mean and variance increase linearly with time (<span class="math inline">\(E[N_t] = \lambda t\)</span> and <span class="math inline">\(Var[N_t] = \lambda t\)</span>).</li>
<li><strong>Markov Property</strong>: Yes, it satisfies the Markov property due to its independent increments.</li>
<li><strong>Sums of Poisson Processes</strong>: If independent Poisson processes are summed, the result is another Poisson process with a rate equal to the sum of the individual rates.</li>
<li><strong>Thinning of a Poisson Process</strong>: If events in a Poisson process are categorized into types, each type forms its own independent Poisson process, with a rate equal to the original rate multiplied by the probability of that type of event.</li>
</ul></li>
</ol>
<hr />
</div>
<div id="objective-3-derive-the-kolmogorov-equations-for-a-markov-process-with-time-independent-and-timeage-dependent-transition-intensities." class="section level4 hasAnchor" number="9.0.1.3">
<h4><span class="header-section-number">9.0.1.3</span> <strong>Objective 3: Derive the Kolmogorov equations for a Markov process with time independent and time/age dependent transition intensities.</strong><a href="markov-processes.html#objective-3-derive-the-kolmogorov-equations-for-a-markov-process-with-time-independent-and-timeage-dependent-transition-intensities." class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Kolmogorov equations are a set of differential equations that describe how transition probabilities evolve over time for Markov jump processes. They are crucial because in continuous time, the concept of a “one-step probability” is replaced by “transition rates”.</p>
<ol style="list-style-type: decimal">
<li><strong>Transition Rates/Intensities (<span class="math inline">\(\mu_{ij}\)</span>)</strong>:
<ul>
<li>For a continuous-time process, we consider transition probabilities over a very short time interval <span class="math inline">\(h\)</span>. Dividing by <span class="math inline">\(h\)</span> and taking the limit as <span class="math inline">\(h \to 0\)</span> leads to the concept of a <strong>transition rate</strong> (also called transition intensity or force of transition).</li>
<li><span class="math inline">\(\mu_{ij} = \lim_{h \to 0} \frac{P(X_{t+h}=j | X_t=i)}{h}\)</span> for <span class="math inline">\(i \ne j\)</span>. This represents the instantaneous rate of jumping from state <span class="math inline">\(i\)</span> to state <span class="math inline">\(j\)</span>.</li>
<li>Importantly, unlike probabilities, transition rates can take values greater than 1 (e.g., annual recovery rates).</li>
<li>The term <span class="math inline">\(\mu_{ii}\)</span> is defined as minus the sum of the transition rates <em>out</em> of state <span class="math inline">\(i\)</span>: <span class="math inline">\(\mu_{ii} = - \sum_{j \ne i} \mu_{ij}\)</span>. This implies that each row of the <strong>generator matrix</strong> sums to zero.</li>
</ul></li>
<li><strong>Time-Homogeneous Markov Jump Processes</strong>:
<ul>
<li><strong>Definition</strong>: A Markov jump process is time-homogeneous if its transition probabilities <span class="math inline">\(p_{ij}(t)\)</span> (the probability of moving from state <span class="math inline">\(i\)</span> to state <span class="math inline">\(j\)</span> in time <span class="math inline">\(t\)</span>) depend <em>only</em> on the length of the time interval <span class="math inline">\(t\)</span>, and not on the absolute starting time <span class="math inline">\(s\)</span> or ending time <span class="math inline">\(t+s\)</span>. This implies constant transition rates.</li>
<li><strong>Chapman-Kolmogorov Equations</strong>: For time-homogeneous processes, these are <span class="math inline">\(p_{ij}(s+t) = \sum_{k \in S} p_{ik}(s) p_{kj}(t)\)</span> for all <span class="math inline">\(s, t &gt; 0\)</span>. The derivation is identical to the discrete-time case.</li>
<li><strong>Kolmogorov’s Forward Differential Equations</strong>: These describe how the probabilities of being in different states evolve <em>forward</em> in time.
<ul>
<li><strong>Component form</strong>: <span class="math inline">\(\frac{d p_{ij}(t)}{dt} = \sum_{k \in S} p_{ik}(t) \mu_{kj}\)</span> for all <span class="math inline">\(i, j\)</span>.</li>
<li><strong>Matrix form</strong>: <span class="math inline">\(\frac{d \mathbf{P}(t)}{dt} = \mathbf{P}(t) \mathbf{A}\)</span>, where <span class="math inline">\(\mathbf{P}(t)\)</span> is the transition matrix and <span class="math inline">\(\mathbf{A}\)</span> is the generator matrix (with entries <span class="math inline">\(\mu_{kj}\)</span>).</li>
</ul></li>
<li><strong>Kolmogorov’s Backward Differential Equations</strong>: These describe the evolution of probabilities by considering the state at the <em>beginning</em> of a short interval.
<ul>
<li><strong>Component form</strong>: <span class="math inline">\(\frac{d p_{ij}(t)}{dt} = \sum_{k \in S} \mu_{ik} p_{kj}(t)\)</span> for all <span class="math inline">\(i, j\)</span>.</li>
<li><strong>Matrix form</strong>: <span class="math inline">\(\frac{d \mathbf{P}(t)}{dt} = \mathbf{A} \mathbf{P}(t)\)</span>.</li>
</ul></li>
</ul></li>
<li><strong>Time-Inhomogeneous Markov Jump Processes</strong>:
<ul>
<li><strong>Definition</strong>: A Markov jump process is time-inhomogeneous if its transition rates <span class="math inline">\(\mu_{ij}(t)\)</span> (and thus its transition probabilities <span class="math inline">\(p_{ij}(s,t)\)</span>) depend not only on the length of the time interval but also on the absolute times <span class="math inline">\(s\)</span> and <span class="math inline">\(t\)</span>. This is often the case when rates depend on age.</li>
<li><strong>Chapman-Kolmogorov Equations</strong>: These remain in the same format but with time-dependent probabilities: <span class="math inline">\(p_{ij}(s,t) = \sum_{k \in S} p_{ik}(s,u) p_{kj}(u,t)\)</span> for all <span class="math inline">\(s \le u \le t\)</span>. In matrix form: <span class="math inline">\(\mathbf{P}(s,t) = \mathbf{P}(s,u) \mathbf{P}(u,t)\)</span>.</li>
<li><strong>Kolmogorov’s Forward Differential Equations</strong>:
<ul>
<li><strong>Component form</strong>: <span class="math inline">\(\frac{\partial p_{ij}(s,t)}{\partial t} = \sum_{k \in S} p_{ik}(s,t) \mu_{kj}(t)\)</span> for all <span class="math inline">\(i, j\)</span>.</li>
<li><strong>Matrix form</strong>: <span class="math inline">\(\frac{\partial \mathbf{P}(s,t)}{\partial t} = \mathbf{P}(s,t) \mathbf{A}(t)\)</span>, where <span class="math inline">\(\mathbf{A}(t)\)</span> is the time-dependent generator matrix.</li>
</ul></li>
<li><strong>Kolmogorov’s Backward Differential Equations</strong>: This is the <em>only</em> one of the Kolmogorov differential equations where the derivative is taken with respect to the <em>starting time</em> <span class="math inline">\(s\)</span>.
<ul>
<li><strong>Component form</strong>: <span class="math inline">\(\frac{\partial p_{ij}(s,t)}{\partial s} = - \sum_{k \in S} \mu_{ik}(s) p_{kj}(s,t)\)</span> for all <span class="math inline">\(i, j\)</span>.</li>
<li><strong>Matrix form</strong>: <span class="math inline">\(\frac{\partial \mathbf{P}(s,t)}{\partial s} = - \mathbf{A}(s) \mathbf{P}(s,t)\)</span>.</li>
</ul></li>
<li><strong>Integrated Forms of Kolmogorov Equations</strong>: These forms express transition probabilities as integrals involving transition rates and are particularly useful when rates are time-dependent or for conditioning on the first/last jump. For example, the probability of staying in state <span class="math inline">\(i\)</span> from <span class="math inline">\(s\)</span> to <span class="math inline">\(t\)</span> is <span class="math inline">\(p_{ii}(s,t) = \exp\left(-\int_s^t \lambda_i(u) du\right)\)</span>, where <span class="math inline">\(\lambda_i(u)\)</span> is the total force of transition out of state <span class="math inline">\(i\)</span> at time <span class="math inline">\(u\)</span>.</li>
</ul></li>
</ol>
<hr />
</div>
<div id="objective-4-solve-the-kolmogorov-equations-in-simple-cases." class="section level4 hasAnchor" number="9.0.1.4">
<h4><span class="header-section-number">9.0.1.4</span> <strong>Objective 4: Solve the Kolmogorov equations in simple cases.</strong><a href="markov-processes.html#objective-4-solve-the-kolmogorov-equations-in-simple-cases." class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Solving these differential equations provides expressions for transition probabilities. While complex in general, simpler cases often involve constant transition rates.</p>
<ol style="list-style-type: decimal">
<li><strong>Two-State Markov Model (Alive-Dead)</strong>:
<ul>
<li>This is a fundamental example where transition is only in one direction (alive to dead).</li>
<li><strong>Assumptions</strong>: It relies on the Markov assumption, a specific form for the probability of death in a short interval (<span class="math inline">\(\mu_{x+t} h + o(h)\)</span>), and constant force of mortality <span class="math inline">\(\mu\)</span> over a short period.</li>
<li><strong>Kolmogorov Forward Equation Derivation</strong>: By considering <span class="math inline">\(p_{x}(t+h)\)</span> and using the Markov assumption and the short-interval probability, one can derive <span class="math inline">\(\frac{\partial p_x(t)}{\partial t} = -p_x(t)\mu_{x+t}\)</span>.</li>
<li><strong>Solution for Constant Force</strong>: If <span class="math inline">\(\mu_{x+t}\)</span> is a constant <span class="math inline">\(\mu\)</span>, the solution is <span class="math inline">\(p_x(t) = e^{-\mu t}\)</span>. This is the probability of a life aged <span class="math inline">\(x\)</span> surviving for time <span class="math inline">\(t\)</span>. This result is consistent with survival models formulated in terms of lifetime distributions.</li>
</ul></li>
<li><strong>General Approach to Solving Simple Cases</strong>:
<ul>
<li>For time-homogeneous processes, the solution to <span class="math inline">\(\frac{d \mathbf{P}(t)}{dt} = \mathbf{A} \mathbf{P}(t)\)</span> or <span class="math inline">\(\mathbf{P}(t) \mathbf{A}\)</span> often involves the matrix exponential, <span class="math inline">\(e^{\mathbf{A}t}\)</span>. While explicit calculation of matrix exponentials can be complex, many problems involve small matrices or specific structures that allow for direct solution using standard differential equation techniques.</li>
<li>For instance, in a two-state machine model (‘being repaired’ (0) and ‘working’ (1)), solving the forward differential equation for <span class="math inline">\(P_{00}(t)\)</span> (probability of being in state 0 at time <span class="math inline">\(t\)</span> given starting in state 0) might lead to an expression like <span class="math inline">\(P_{00}(t) = \frac{1}{5} + \frac{4}{5}e^{-5t}\)</span>.</li>
<li>The “Poisson process revisited” section provides a direct example where the specific structure of the generator matrix simplifies the forward equations to a form that can be solved directly, yielding the Poisson distribution for <span class="math inline">\(p_{ij}(t)\)</span>.</li>
</ul></li>
</ol>
<hr />
</div>
<div id="objective-5-state-the-kolmogorov-equations-for-a-model-where-the-transition-intensities-depend-not-only-on-agetime-but-also-on-the-duration-of-stay-in-one-or-more-states." class="section level4 hasAnchor" number="9.0.1.5">
<h4><span class="header-section-number">9.0.1.5</span> <strong>Objective 5: State the Kolmogorov equations for a model where the transition intensities depend not only on age/time, but also on the duration of stay in one or more states.</strong><a href="markov-processes.html#objective-5-state-the-kolmogorov-equations-for-a-model-where-the-transition-intensities-depend-not-only-on-agetime-but-also-on-the-duration-of-stay-in-one-or-more-states." class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Duration dependence means that the transition intensities are not just a function of calendar time or age, but also of how long an individual has been in their current state.</p>
<ol style="list-style-type: decimal">
<li><strong>Concept of Duration Dependence</strong>:
<ul>
<li>In real-world scenarios, a person’s risk of transitioning might depend on how long they have been in their current state. For example, the probability of recovery from a sickness might change with the duration of the illness.</li>
<li>Such models are more complex because the Markov property, which states that future probabilities depend <em>only</em> on the current state, might be violated if “current state” doesn’t implicitly contain information about “duration of stay”.</li>
</ul></li>
<li><strong>Handling Duration Dependence</strong>:
<ul>
<li>The standard approach to incorporate duration dependence while retaining the Markov property is to <strong>expand the state space</strong>. Instead of a single “Sick” state, you might have “Sick (duration 0-1 year)”, “Sick (duration 1-2 years)”, etc.. Each of these new states is now distinct, and the transition probabilities from them depend only on that specific state, satisfying the Markov property.</li>
</ul></li>
<li><strong>Kolmogorov Equations with Duration Dependence</strong>:
<ul>
<li>When duration dependence is incorporated by expanding the state space, the general form of the Kolmogorov forward and backward differential equations (as described in Objective 3 for time-inhomogeneous processes) still applies. However, the generator matrix <span class="math inline">\(\mathbf{A}(t)\)</span> will become much larger and its entries will reflect the transitions between the expanded states, possibly still depending on age/time <span class="math inline">\(t\)</span> and now implicitly capturing duration through the state definition.</li>
<li>For instance, for a probability <span class="math inline">\(P(X_t = H | X_s = S, C_s = w)\)</span>, where <span class="math inline">\(C_s\)</span> is the current holding time in state S at time s, the integral form of the Kolmogorov backward equation can be written as an integral over future possible transitions, incorporating the duration-dependent rates.</li>
</ul></li>
</ol>
<hr />
</div>
<div id="objective-6-describe-sickness-and-marriage-models-in-terms-of-duration-dependent-markov-processes-and-describe-other-simple-applications." class="section level4 hasAnchor" number="9.0.1.6">
<h4><span class="header-section-number">9.0.1.6</span> <strong>Objective 6: Describe sickness and marriage models in terms of duration dependent Markov processes and describe other simple applications.</strong><a href="markov-processes.html#objective-6-describe-sickness-and-marriage-models-in-terms-of-duration-dependent-markov-processes-and-describe-other-simple-applications." class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Markov processes are incredibly versatile for modelling various demographic and insurance-related phenomena.</p>
<ol style="list-style-type: decimal">
<li><strong>Sickness Models</strong>:
<ul>
<li><strong>Health-Sickness-Death (HSD) Model</strong>: A common three-state model (Healthy (H), Sick (S), Dead (D)) illustrates transitions between these health states.
<ul>
<li><strong>Duration Dependence in Sickness</strong>: The rate of recovery from sickness (S to H) or mortality while sick (S to D) might depend on how long a person has been sick. For example, the longer someone is sick, the less likely they are to recover, or their mortality rate might increase.</li>
<li>To model this while maintaining the Markov property, you’d subdivide the “Sick” state, perhaps into “Recently Sick,” “Moderately Sick,” “Long-term Sick,” etc.. This expansion of the state space allows the model to “remember” the duration of sickness within the current state definition.</li>
</ul></li>
</ul></li>
<li><strong>Marriage Models</strong>:
<ul>
<li>These models track an individual’s marital status over time, which can include states like bachelor/spinster (never married) (B), married (M), widowed (W), divorced (D), and dead (<span class="math inline">\(\Delta\)</span>).</li>
<li><strong>Duration Dependence in Marriage</strong>: Transition rates might depend on the duration of a marriage (e.g., divorce rates could be higher in early years of marriage, then stabilize) or the duration of widowhood/divorce.</li>
<li>Similar to sickness models, subdividing states like “Married” into “Married (duration &lt;5 years)”, “Married (duration $$5 years)” would allow for duration-dependent transition rates while preserving the Markov property.</li>
</ul></li>
<li><strong>Other Simple Applications</strong>:
<ul>
<li><strong>Machine Status</strong>: Modelling a machine that transitions between “working” and “being repaired” states, with specific breakdown and repair rates.</li>
<li><strong>Insurance Claims Processing</strong>: Tracking the progress of claims through different stages like “awaiting classification,” “under investigation,” “awaiting further details,” “settled”.</li>
<li><strong>No Claims Discount (NCD) Systems</strong>: While typically modelled as Markov chains (discrete time), the underlying continuous transitions (e.g., probability of a claim occurring at any moment) can be conceptualized as a jump process.</li>
<li><strong>Population Models</strong>: Tracking populations through various states like healthy, infected, dead, or even animal populations with birth/death/infection rates.</li>
<li><strong>Company Credit Ratings</strong>: Assessing the creditworthiness of debt, with ratings like A, B, D (defaulted), and transitions between them.</li>
</ul></li>
</ol>
<hr />
</div>
<div id="objective-7-demonstrate-how-markov-jump-processes-can-be-used-as-a-tool-for-modelling-and-how-they-can-be-simulated." class="section level4 hasAnchor" number="9.0.1.7">
<h4><span class="header-section-number">9.0.1.7</span> <strong>Objective 7: Demonstrate how Markov jump processes can be used as a tool for modelling and how they can be simulated.</strong><a href="markov-processes.html#objective-7-demonstrate-how-markov-jump-processes-can-be-used-as-a-tool-for-modelling-and-how-they-can-be-simulated." class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Markov jump processes are powerful modelling tools, and their simulation allows for forecasting and scenario analysis.</p>
<ol style="list-style-type: decimal">
<li><p><strong>Modelling with Markov Jump Processes</strong>:</p>
<ul>
<li>The <strong>generator matrix</strong> <span class="math inline">\(\mathbf{A}\)</span> (or <span class="math inline">\(\mathbf{A}(t)\)</span> for time-inhomogeneous processes) is the fundamental element, fully characterizing the process’s distribution.</li>
<li>Maximum Likelihood Estimators (MLEs) for constant transition rates <span class="math inline">\(\mu_{ij}\)</span> are typically calculated as the number of observed transitions from state <span class="math inline">\(i\)</span> to state <span class="math inline">\(j\)</span> (<span class="math inline">\(n_{ij}\)</span>) divided by the total observed waiting time in state <span class="math inline">\(i\)</span> (<span class="math inline">\(t_i\)</span>): <span class="math inline">\(\hat{\mu}_{ij} = n_{ij} / t_i\)</span>. These estimators are asymptotically normally distributed.</li>
<li>The model assumes constant transition rates (for time-homogeneous models) and that the Markov property holds.</li>
</ul></li>
<li><p><strong>Simulation of Markov Jump Processes</strong>:
Simulating a Markov jump process means generating a sample path, which involves two main components: the sequence of states visited and the time spent in each state. There are two main approaches.</p>
<ul>
<li><strong>Jump Chain (Embedded Chain)</strong>:
<ul>
<li>This is the sequence of states that the Markov jump process enters, <em>ignoring the time spent in each state</em>. Its time set is discrete, comprising only the times at which transitions occur.</li>
<li>The key insight is that the jump chain itself possesses the Markov property and is a <strong>Markov chain</strong>. This is because the destination of the next jump (when leaving state <span class="math inline">\(i\)</span>) is independent of the holding time and depends only on the current state <span class="math inline">\(i\)</span>.</li>
<li>The transition probability from state <span class="math inline">\(i\)</span> to state <span class="math inline">\(j\)</span> in the jump chain is given by the ratio of the transition rate from <span class="math inline">\(i\)</span> to <span class="math inline">\(j\)</span> to the total force of transition out of state <span class="math inline">\(i\)</span>: <span class="math inline">\(p_{ij} = \mu_{ij} / \lambda_i\)</span>.</li>
<li>Questions “dealing solely with the sequence of states visited… can be answered equally well with reference to the jump chain… Questions dealing with the time taken to visit a state, however, are likely to have very different answers… and are only accessible using the theory of Markov jump processes”.</li>
</ul></li>
<li><strong>Approximate Method of Simulation</strong>:
<ul>
<li>“Divide time into very short intervals of width <span class="math inline">\(h\)</span>, say, where <span class="math inline">\(\mu_{ij}h\)</span> is much smaller than 1 for each <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>”.</li>
<li>Then, the transition matrix <span class="math inline">\(\mathbf{P}(h)\)</span> of the Markov chain (over this short interval) is approximated by <span class="math inline">\(\mathbf{P}(h) \approx \mathbf{I} + h\mathbf{A}\)</span>.</li>
<li>A discrete-time Markov chain <span class="math inline">\(\{Y_n\}\)</span> is simulated using these approximate transition probabilities, and the Markov jump process <span class="math inline">\(X_t\)</span> is defined as <span class="math inline">\(Y_{[t/h]}\)</span>.</li>
<li><strong>Limitations</strong>: This method is “not very satisfactory” because “its long-term distribution may differ significantly from that of the process being modelled” due to accumulating errors from the approximation.</li>
</ul></li>
<li><strong>Exact Method of Simulation</strong>:
<ul>
<li>This method leverages the structural decomposition of the jump process.</li>
<li><strong>Step 1</strong>: Simulate the jump chain of the process as a Markov chain using the transition probabilities <span class="math inline">\(p_{ij} = \mu_{ij} / \lambda_i\)</span>. This generates the sequence of states visited (<span class="math inline">\(\hat{X}_0, \hat{X}_1, \dots\)</span>).</li>
<li><strong>Step 2</strong>: Simulate the holding times. For each state <span class="math inline">\(\hat{X}_n\)</span> entered, the holding time <span class="math inline">\(T_n\)</span> is an independent Exponential random variable with rate parameter <span class="math inline">\(\lambda_{\hat{X}_n}\)</span> (the total force of transition out of that state).</li>
<li>By summing these holding times, you obtain the exact times at which the Markov process jumps between states.</li>
<li>For time-inhomogeneous processes, determining the density function of the time until the next transition and its destination is “in principle possible” for exact simulation, but “cumbersome in the extreme” in practice, making the approximate method more usual.</li>
</ul></li>
<li><strong>R Implementation</strong>: The <code>markovchain</code> package in R can be used to create Markov chain objects for jump chains, calculate n-step transition probabilities, and compute expected rewards. R functions can also calculate survival probabilities and expected life directly, often through numerical integration for time-inhomogeneous models.</li>
</ul></li>
</ol>
<hr />
<p>This comprehensive overview should equip you with a robust understanding of Markov processes, their underlying theory, practical applications, and the vital techniques for their analysis and simulation. Keep practicing the exam-style questions to solidify these concepts!</p>
</div>
</div>
</div>
<div id="practice-09" class="section level2 unnumbered hasAnchor">
<h2><code>R</code> Practice<a href="markov-processes.html#practice-09" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>TO ADD R EXAMPLE ABOUT MARKOV PROCESSES HERE</strong></p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="markov-chains.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="survival-models.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": true,
    "facebook": false,
    "twitter": true,
    "linkedin": true,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["twitter", "linkedin"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": "https://github.com/priyam0k/CS2/edit/main/09-markov-processes.Rmd",
    "text": "Edit"
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": null,
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "section",
    "scroll_highlight": true
  },
  "toolbar": {
    "position": "fixed"
  },
  "info": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
