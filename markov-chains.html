<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 8 Markov chains | Risk Modelling and Survival Analysis</title>
  <meta name="description" content="A modified version of the book on Risk Modelling and Survival Analysis, with additional notes and examples." />
  <meta name="generator" content="bookdown 0.43 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 8 Markov chains | Risk Modelling and Survival Analysis" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="A modified version of the book on Risk Modelling and Survival Analysis, with additional notes and examples." />
  <meta name="github-repo" content="priyam0k/CS2" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 8 Markov chains | Risk Modelling and Survival Analysis" />
  
  <meta name="twitter:description" content="A modified version of the book on Risk Modelling and Survival Analysis, with additional notes and examples." />
  

<meta name="author" content="Priyam" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  <link rel="shortcut icon" href="img/favicon.ico" type="image/x-icon" />
<link rel="prev" href="stochastic-processes.html"/>
<link rel="next" href="markov-processes.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Risk Modelling and Survival Analysis</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#motivation"><i class="fa fa-check"></i>Motivation</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="r-setup.html"><a href="r-setup.html"><i class="fa fa-check"></i><b>1</b> R Setup</a>
<ul>
<li class="chapter" data-level="1.1" data-path="r-setup.html"><a href="r-setup.html#preparing-your-environment"><i class="fa fa-check"></i><b>1.1</b> Preparing your environment</a></li>
<li class="chapter" data-level="1.2" data-path="r-setup.html"><a href="r-setup.html#basic-interations-with-r"><i class="fa fa-check"></i><b>1.2</b> Basic interations with R</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="loss-distributions.html"><a href="loss-distributions.html"><i class="fa fa-check"></i><b>2</b> Loss distributions</a>
<ul>
<li class="chapter" data-level="" data-path="loss-distributions.html"><a href="loss-distributions.html#objectives-loss-distributions"><i class="fa fa-check"></i>Learning Objectives</a></li>
<li class="chapter" data-level="" data-path="loss-distributions.html"><a href="loss-distributions.html#theory-loss-distributions"><i class="fa fa-check"></i>Theory</a></li>
<li class="chapter" data-level="" data-path="loss-distributions.html"><a href="loss-distributions.html#practice-loss-distributions"><i class="fa fa-check"></i><code>R</code> Practice</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="compound-loss-distributions.html"><a href="compound-loss-distributions.html"><i class="fa fa-check"></i><b>3</b> Compound loss distributions</a>
<ul>
<li class="chapter" data-level="" data-path="compound-loss-distributions.html"><a href="compound-loss-distributions.html#objectives-compound-loss-distributions"><i class="fa fa-check"></i>Learning Objectives</a></li>
<li class="chapter" data-level="" data-path="compound-loss-distributions.html"><a href="compound-loss-distributions.html#theory-compound-loss-distributions"><i class="fa fa-check"></i>Theory</a>
<ul>
<li class="chapter" data-level="3.0.1" data-path="compound-loss-distributions.html"><a href="compound-loss-distributions.html#chapter-compound-loss-distributions"><i class="fa fa-check"></i><b>3.0.1</b> <strong>Chapter: Compound Loss Distributions</strong></a></li>
</ul></li>
<li class="chapter" data-level="" data-path="compound-loss-distributions.html"><a href="compound-loss-distributions.html#practice-compound-loss-distributions"><i class="fa fa-check"></i><code>R</code> Practice</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="copulas.html"><a href="copulas.html"><i class="fa fa-check"></i><b>4</b> Copulas</a>
<ul>
<li class="chapter" data-level="" data-path="copulas.html"><a href="copulas.html#objectives-copulas"><i class="fa fa-check"></i>Learning Objectives</a></li>
<li class="chapter" data-level="" data-path="copulas.html"><a href="copulas.html#theory-copulas"><i class="fa fa-check"></i>Theory</a>
<ul>
<li class="chapter" data-level="4.0.1" data-path="copulas.html"><a href="copulas.html#chapter-copulas-modelling-dependency-structures"><i class="fa fa-check"></i><b>4.0.1</b> <strong>Chapter: Copulas – Modelling Dependency Structures</strong></a></li>
</ul></li>
<li class="chapter" data-level="" data-path="copulas.html"><a href="copulas.html#practice-copulas"><i class="fa fa-check"></i><code>R</code> Practice</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="extreme-value-theory.html"><a href="extreme-value-theory.html"><i class="fa fa-check"></i><b>5</b> Extreme value theory</a>
<ul>
<li class="chapter" data-level="" data-path="extreme-value-theory.html"><a href="extreme-value-theory.html#objectives-evt"><i class="fa fa-check"></i>Learning Objectives</a></li>
<li class="chapter" data-level="" data-path="extreme-value-theory.html"><a href="extreme-value-theory.html#theory-evt"><i class="fa fa-check"></i>Theory</a>
<ul>
<li class="chapter" data-level="5.0.1" data-path="extreme-value-theory.html"><a href="extreme-value-theory.html#chapter-16-extreme-value-theory"><i class="fa fa-check"></i><b>5.0.1</b> Chapter 16: Extreme Value Theory</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="extreme-value-theory.html"><a href="extreme-value-theory.html#practice-evt"><i class="fa fa-check"></i><code>R</code> Practice</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="time-series.html"><a href="time-series.html"><i class="fa fa-check"></i><b>6</b> Time series</a>
<ul>
<li class="chapter" data-level="" data-path="time-series.html"><a href="time-series.html#objectives-time-series"><i class="fa fa-check"></i>Learning Objectives</a></li>
<li class="chapter" data-level="6.1" data-path="time-series.html"><a href="time-series.html#theory-time-series"><i class="fa fa-check"></i><b>6.1</b> Theory</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="time-series.html"><a href="time-series.html#time-series-a-deep-dive-for-cs2-actuarial-professionals"><i class="fa fa-check"></i><b>6.1.1</b> Time Series: A Deep Dive for CS2 Actuarial Professionals</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="time-series.html"><a href="time-series.html#practice-time-series"><i class="fa fa-check"></i><code>R</code> Practice</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="stochastic-processes.html"><a href="stochastic-processes.html"><i class="fa fa-check"></i><b>7</b> Stochastic processes</a>
<ul>
<li class="chapter" data-level="" data-path="stochastic-processes.html"><a href="stochastic-processes.html#objectives-stochastic-processes"><i class="fa fa-check"></i>Learning Objectives</a></li>
<li class="chapter" data-level="" data-path="stochastic-processes.html"><a href="stochastic-processes.html#theory-stochastic-processes"><i class="fa fa-check"></i>Theory</a>
<ul>
<li class="chapter" data-level="7.0.1" data-path="stochastic-processes.html"><a href="stochastic-processes.html#chapter-1-stochastic-processes"><i class="fa fa-check"></i><b>7.0.1</b> Chapter 1: Stochastic Processes</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="stochastic-processes.html"><a href="stochastic-processes.html#practice-stochastic-processes"><i class="fa fa-check"></i><code>R</code> Practice</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="markov-chains.html"><a href="markov-chains.html"><i class="fa fa-check"></i><b>8</b> Markov chains</a>
<ul>
<li class="chapter" data-level="" data-path="markov-chains.html"><a href="markov-chains.html#objectives-markov-chains"><i class="fa fa-check"></i>Learning Objectives</a></li>
<li class="chapter" data-level="" data-path="markov-chains.html"><a href="markov-chains.html#theory-markov-chains"><i class="fa fa-check"></i>Theory</a>
<ul>
<li class="chapter" data-level="8.0.1" data-path="markov-chains.html"><a href="markov-chains.html#chapter-markov-chains-a-comprehensive-overview"><i class="fa fa-check"></i><b>8.0.1</b> Chapter: Markov Chains – A Comprehensive Overview</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="markov-chains.html"><a href="markov-chains.html#practice-markov-chains"><i class="fa fa-check"></i><code>R</code> Practice</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="markov-processes.html"><a href="markov-processes.html"><i class="fa fa-check"></i><b>9</b> Markov processes</a>
<ul>
<li class="chapter" data-level="" data-path="markov-processes.html"><a href="markov-processes.html#objectives-09"><i class="fa fa-check"></i>Learning Objectives</a></li>
<li class="chapter" data-level="" data-path="markov-processes.html"><a href="markov-processes.html#theory-09"><i class="fa fa-check"></i>Theory</a></li>
<li class="chapter" data-level="9.1" data-path="markov-processes.html"><a href="markov-processes.html#features-of-a-markov-process-model"><i class="fa fa-check"></i><b>9.1</b> Features of a Markov process model</a></li>
<li class="chapter" data-level="9.2" data-path="markov-processes.html"><a href="markov-processes.html#poisson-process"><i class="fa fa-check"></i><b>9.2</b> Poisson process</a></li>
<li class="chapter" data-level="9.3" data-path="markov-processes.html"><a href="markov-processes.html#kolmogorov-equations-for-a-markov-process"><i class="fa fa-check"></i><b>9.3</b> Kolmogorov equations for a Markov process</a></li>
<li class="chapter" data-level="9.4" data-path="markov-processes.html"><a href="markov-processes.html#solving-kolmogorv-equations"><i class="fa fa-check"></i><b>9.4</b> Solving Kolmogorv equations</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="markov-processes.html"><a href="markov-processes.html#simple-cases"><i class="fa fa-check"></i><b>9.4.1</b> Simple cases</a></li>
<li class="chapter" data-level="9.4.2" data-path="markov-processes.html"><a href="markov-processes.html#more-general-cases"><i class="fa fa-check"></i><b>9.4.2</b> More general cases</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="markov-processes.html"><a href="markov-processes.html#sickness-and-marriage-models"><i class="fa fa-check"></i><b>9.5</b> Sickness and marriage models</a></li>
<li class="chapter" data-level="9.6" data-path="markov-processes.html"><a href="markov-processes.html#markov-jump-process"><i class="fa fa-check"></i><b>9.6</b> Markov jump process</a>
<ul>
<li class="chapter" data-level="9.6.1" data-path="markov-processes.html"><a href="markov-processes.html#simulating-a-markov-jump-process"><i class="fa fa-check"></i><b>9.6.1</b> Simulating a Markov jump process</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="markov-processes.html"><a href="markov-processes.html#practice-09"><i class="fa fa-check"></i><code>R</code> Practice</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="survival-models.html"><a href="survival-models.html"><i class="fa fa-check"></i><b>10</b> Survival models</a></li>
<li class="chapter" data-level="11" data-path="lifetime-distributions.html"><a href="lifetime-distributions.html"><i class="fa fa-check"></i><b>11</b> Lifetime distributions</a></li>
<li class="chapter" data-level="12" data-path="transition-intensities.html"><a href="transition-intensities.html"><i class="fa fa-check"></i><b>12</b> Estimating transition intensities</a></li>
<li class="chapter" data-level="13" data-path="graduation.html"><a href="graduation.html"><i class="fa fa-check"></i><b>13</b> Graduation</a></li>
<li class="chapter" data-level="14" data-path="mortality-projection.html"><a href="mortality-projection.html"><i class="fa fa-check"></i><b>14</b> Mortality projection</a></li>
<li class="chapter" data-level="15" data-path="machine-learning.html"><a href="machine-learning.html"><i class="fa fa-check"></i><b>15</b> Machine learning</a>
<ul>
<li class="chapter" data-level="" data-path="machine-learning.html"><a href="machine-learning.html#objectives-15"><i class="fa fa-check"></i>Learning Objectives</a></li>
<li class="chapter" data-level="" data-path="machine-learning.html"><a href="machine-learning.html#theory-15"><i class="fa fa-check"></i>Theory</a></li>
<li class="chapter" data-level="15.1" data-path="machine-learning.html"><a href="machine-learning.html#machine-learning-topics"><i class="fa fa-check"></i><b>15.1</b> Machine learning topics</a></li>
<li class="chapter" data-level="15.2" data-path="machine-learning.html"><a href="machine-learning.html#machine-learning-from-data"><i class="fa fa-check"></i><b>15.2</b> Machine learning from data</a></li>
<li class="chapter" data-level="15.3" data-path="machine-learning.html"><a href="machine-learning.html#supervised-machine-learning"><i class="fa fa-check"></i><b>15.3</b> Supervised machine learning</a></li>
<li class="chapter" data-level="15.4" data-path="machine-learning.html"><a href="machine-learning.html#unsupervised-machine-learning"><i class="fa fa-check"></i><b>15.4</b> Unsupervised machine learning</a></li>
<li class="chapter" data-level="15.5" data-path="machine-learning.html"><a href="machine-learning.html#penalised-regression"><i class="fa fa-check"></i><b>15.5</b> Penalised regression</a></li>
<li class="chapter" data-level="15.6" data-path="machine-learning.html"><a href="machine-learning.html#decision-trees"><i class="fa fa-check"></i><b>15.6</b> Decision trees</a></li>
<li class="chapter" data-level="15.7" data-path="machine-learning.html"><a href="machine-learning.html#perspectives-of-non-actuarial-professionals"><i class="fa fa-check"></i><b>15.7</b> Perspectives of non-actuarial professionals</a></li>
<li class="chapter" data-level="" data-path="machine-learning.html"><a href="machine-learning.html#practice-15"><i class="fa fa-check"></i><code>R</code> Practice</a></li>
</ul></li>
<li class="divider"></li>
<li>Adapted by <a href="https://github.com/priyam0k">Priyam</a> from the original by <a href="https://github.com/agarbiak" target="blank">Alex Garbiak</a>.</li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Risk Modelling and Survival Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="markov-chains" class="section level1 hasAnchor" number="8">
<h1><span class="header-section-number">Chapter 8</span> Markov chains<a href="markov-chains.html#markov-chains" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="objectives-markov-chains" class="section level2 unnumbered hasAnchor">
<h2>Learning Objectives<a href="markov-chains.html#objectives-markov-chains" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ol style="list-style-type: decimal">
<li>State the essential features of a Markov chain model.</li>
<li>State the Chapman-Kolmogorov equations that represent a Markov chain.</li>
<li>Calculate the stationary distribution for a Markov chain in simple cases.</li>
<li>Describe a system of frequency based experience rating in terms of a Markov chain and describe other simple applications.</li>
<li>Describe a time-inhomogeneous Markov chain model and describe simple applications.</li>
<li>Demonstrate how Markov chains can be used as a tool for modelling and how they can be simulated.</li>
</ol>
</div>
<div id="theory-markov-chains" class="section level2 unnumbered hasAnchor">
<h2>Theory<a href="markov-chains.html#theory-markov-chains" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="chapter-markov-chains-a-comprehensive-overview" class="section level3 hasAnchor" number="8.0.1">
<h3><span class="header-section-number">8.0.1</span> Chapter: Markov Chains – A Comprehensive Overview<a href="markov-chains.html#chapter-markov-chains-a-comprehensive-overview" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Markov chains are a cornerstone of stochastic processes in actuarial science, particularly vital for modelling systems that evolve over time in a probabilistic manner. They provide a powerful framework for understanding and predicting future states based solely on the present.</p>
<hr />
<div id="state-the-essential-features-of-a-markov-chain-model." class="section level4 hasAnchor" number="8.0.1.1">
<h4><span class="header-section-number">8.0.1.1</span> 1. State the essential features of a Markov chain model.<a href="markov-chains.html#state-the-essential-features-of-a-markov-chain-model." class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>A Markov chain is a specific type of <strong>stochastic process</strong> characterized by three essential features: a discrete time set, a discrete state space, and the fundamental Markov property.</p>
<ul>
<li><strong>Stochastic Process Foundation</strong>: At its core, a stochastic process is a family or set of ordered random variables, typically indexed by time. <span class="math inline">\(X_t\)</span> represents the value of the process at time <span class="math inline">\(t\)</span>.</li>
<li><strong>Discrete Time Set</strong>: For a Markov chain, observations or changes occur only at specific, separated points in time. This means our time index <span class="math inline">\(t\)</span> (or <span class="math inline">\(n\)</span>) belongs to a discrete set, such as <span class="math inline">\(\{0, 1, 2, \dots\}\)</span>.</li>
<li><strong>Discrete State Space</strong>: The set of all possible values that the random variable <span class="math inline">\(X_t\)</span> can take is finite or countably infinite. For instance, if we’re modelling a No Claims Discount (NCD) system, the states might be {0% discount, 25% discount, 50% discount}, which are discrete categories.</li>
<li><strong>The Markov Property</strong>: This is the defining characteristic. It states that the future development of the process, or the probability distribution of future values, depends <em>only</em> on the current state and not on the entire history of how that state was reached.
<ul>
<li>Mathematically, for a discrete state space, this means: <span class="math inline">\(P[X_n = j | X_m = i_m, X_{m-1} = i_{m-1}, \dots, X_0 = i_0] = P[X_n = j | X_m = i_m]\)</span> for all <span class="math inline">\(n &gt; m\)</span> and all states.</li>
<li>The interpretation is profound: given the present, the past is irrelevant for predicting the future. This simplifies modelling considerably.</li>
</ul></li>
<li><strong>Transition Probabilities and Matrix</strong>: The “key objects” describing a Markov chain are its <strong>transition probabilities</strong>. For a time-homogeneous Markov chain (which we’ll discuss next), <span class="math inline">\(p_{ij}\)</span> denotes the probability of moving from state <span class="math inline">\(i\)</span> to state <span class="math inline">\(j\)</span> in one step. These probabilities are organised into a <strong>transition matrix, P</strong>, where <span class="math inline">\(P_{ij} = p_{ij}\)</span>. Each row of the transition matrix must sum to 1, representing the certainty of moving to <em>some</em> state.</li>
<li><strong>Transition Graph</strong>: A visual representation of a Markov chain, where states are nodes and arrows indicate possible transitions (<span class="math inline">\(p_{ij} &gt; 0\)</span>), often with the probability values marked on the arrows.</li>
</ul>
<hr />
</div>
<div id="state-the-chapman-kolmogorov-equations-that-represent-a-markov-chain." class="section level4 hasAnchor" number="8.0.1.2">
<h4><span class="header-section-number">8.0.1.2</span> 2. State the Chapman-Kolmogorov equations that represent a Markov chain.<a href="markov-chains.html#state-the-chapman-kolmogorov-equations-that-represent-a-markov-chain." class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The Chapman-Kolmogorov equations are fundamental to understanding how transition probabilities evolve over multiple steps in a Markov chain. They essentially allow us to calculate multi-step transition probabilities by “chaining together” one-step (or shorter-step) transitions.</p>
<ul>
<li><strong>Formula (Time-Homogeneous Case)</strong>: For a time-homogeneous Markov chain (where one-step transition probabilities are constant over time), the <span class="math inline">\(n\)</span>-step transition probability <span class="math inline">\(p_{ij}^{(n)}\)</span> can be found using an intermediate state <span class="math inline">\(k\)</span> at time <span class="math inline">\(l\)</span> (where <span class="math inline">\(m &lt; l &lt; n\)</span>):
<span class="math inline">\(p_{ij}^{(n-m)} = \sum_{k \in S} p_{ik}^{(l-m)} p_{kj}^{(n-l)}\)</span>.
This means the probability of going from state <span class="math inline">\(i\)</span> to state <span class="math inline">\(j\)</span> in <span class="math inline">\((n-m)\)</span> steps is the sum over all possible intermediate states <span class="math inline">\(k\)</span> of the probability of going from <span class="math inline">\(i\)</span> to <span class="math inline">\(k\)</span> in <span class="math inline">\((l-m)\)</span> steps, multiplied by the probability of going from <span class="math inline">\(k\)</span> to <span class="math inline">\(j\)</span> in <span class="math inline">\((n-l)\)</span> steps.</li>
<li><strong>Matrix Form</strong>: This is where the power of linear algebra shines. The Chapman-Kolmogorov equations simplify beautifully in matrix form:
<span class="math inline">\(P^{(n)} = P^{(l)} P^{(n-l)}\)</span> where <span class="math inline">\(P^{(n)}\)</span> is the <span class="math inline">\(n\)</span>-step transition matrix. For time-homogeneous chains, the <span class="math inline">\(l\)</span>-step transition probability matrix <span class="math inline">\(P^{(l)}\)</span> is simply the <span class="math inline">\(l\)</span>-th power of the one-step transition matrix <span class="math inline">\(P\)</span>: <span class="math inline">\(P^{(l)} = P^l\)</span>. Thus, <span class="math inline">\(P^{(n-m)} = P^{(l-m)} P^{(n-l)}\)</span>.</li>
<li><strong>Proof Intuition</strong>: The proof relies on the Markov property and the law of total probability. It partitions all possible paths from state <span class="math inline">\(i\)</span> at time <span class="math inline">\(m\)</span> to state <span class="math inline">\(j\)</span> at time <span class="math inline">\(n\)</span> by considering all possible intermediate states <span class="math inline">\(k\)</span> at an intermediate time <span class="math inline">\(l\)</span>. The probability of traversing a specific path through <span class="math inline">\(k\)</span> is the product of the probabilities of its segments (due to the Markov property), and summing these products over all possible intermediate <span class="math inline">\(k\)</span> gives the total probability of reaching <span class="math inline">\(j\)</span> from <span class="math inline">\(i\)</span>. While you’re not expected to reproduce the formal proof in exams, understanding its logical foundation is key.</li>
<li><strong>Importance</strong>: These equations fully determine the distribution of a Markov chain once the one-step transition probabilities and the initial probability distribution are specified.</li>
</ul>
<hr />
</div>
<div id="calculate-the-stationary-distribution-for-a-markov-chain-in-simple-cases." class="section level4 hasAnchor" number="8.0.1.3">
<h4><span class="header-section-number">8.0.1.3</span> 3. Calculate the stationary distribution for a Markov chain in simple cases.<a href="markov-chains.html#calculate-the-stationary-distribution-for-a-markov-chain-in-simple-cases." class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The concept of a stationary distribution (also known as an invariant or equilibrium distribution) is crucial for understanding the long-term behaviour of Markov chains.</p>
<ul>
<li><strong>Definition</strong>: A probability distribution <span class="math inline">\(\pi = (\pi_1, \pi_2, \dots, \pi_N)\)</span> is a stationary distribution for a Markov chain with transition matrix <span class="math inline">\(P\)</span> if, when the process is in this distribution, it remains in it after one step (and thus all subsequent steps). The conditions are:
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\pi P = \pi\)</span> (in matrix form, where <span class="math inline">\(\pi\)</span> is a row vector). This means <span class="math inline">\(\pi_j = \sum_{i \in S} \pi_i p_{ij}\)</span> for all states <span class="math inline">\(j\)</span>.</li>
<li><span class="math inline">\(\pi_j \ge 0\)</span> for all <span class="math inline">\(j \in S\)</span> (probabilities must be non-negative).</li>
<li><span class="math inline">\(\sum_{j \in S} \pi_j = 1\)</span> (the probabilities must sum to one).</li>
</ol></li>
<li><strong>Existence and Uniqueness</strong>:
<ul>
<li><strong>Existence</strong>: A Markov chain with a <strong>finite state space</strong> is guaranteed to have <em>at least one</em> stationary probability distribution.</li>
<li><strong>Uniqueness</strong>: For a unique stationary distribution to exist, the chain must also be <strong>irreducible</strong>.
<ul>
<li><strong>Irreducibility</strong>: A Markov chain is irreducible if it’s possible to reach <em>any</em> state <span class="math inline">\(j\)</span> from <em>any other</em> state <span class="math inline">\(i\)</span> (either directly or indirectly). This is often assessed by examining the transition graph for connectivity.</li>
</ul></li>
<li><strong>Convergence</strong>: For the process to <em>converge</em> to this unique stationary distribution in the long run, the chain must also be <strong>aperiodic</strong>.
<ul>
<li><strong>Periodicity</strong>: A state <span class="math inline">\(i\)</span> is periodic with period <span class="math inline">\(d &gt; 1\)</span> if a return to state <span class="math inline">\(i\)</span> is possible only in a number of steps that is a multiple of <span class="math inline">\(d\)</span>. If all states in an irreducible Markov chain have period <span class="math inline">\(d=1\)</span>, the chain is <strong>aperiodic</strong>. If a chain is irreducible, all its states share the same period.</li>
</ul></li>
</ul></li>
<li><strong>Calculation Method</strong>: In simple cases, calculating the stationary distribution involves setting up the system of linear equations from <span class="math inline">\(\pi P = \pi\)</span> and adding the normalisation condition <span class="math inline">\(\sum \pi_j = 1\)</span>. Then, you solve this system of simultaneous equations.
<ul>
<li><strong>Using R</strong>: The <code>steadyStates()</code> function in the <code>markovchain</code> package is very useful for directly calculating the stationary distribution. Alternatively, you can calculate a large power of the transition matrix (e.g., <span class="math inline">\(P^{40}\)</span> or <span class="math inline">\(P^{240}\)</span>) using <code>mc ^ n</code> or <code>Pn(P, n)</code> in base R, and the rows of the resulting matrix will approximate the stationary distribution. This demonstrates the “settling down” behaviour of the chain.</li>
</ul></li>
</ul>
<hr />
</div>
<div id="describe-a-system-of-frequency-based-experience-rating-in-terms-of-a-markov-chain-and-describe-other-simple-applications." class="section level4 hasAnchor" number="8.0.1.4">
<h4><span class="header-section-number">8.0.1.4</span> 4. Describe a system of frequency based experience rating in terms of a Markov chain and describe other simple applications.<a href="markov-chains.html#describe-a-system-of-frequency-based-experience-rating-in-terms-of-a-markov-chain-and-describe-other-simple-applications." class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Markov chains are incredibly versatile for modelling discrete-state, discrete-time systems in actuarial work.</p>
<ul>
<li><strong>No Claims Discount (NCD) Systems in Motor Insurance</strong>: This is a classic and frequently examined application.
<ul>
<li><strong>Mechanism</strong>: Motor insurers offer discounts on premiums based on a policyholder’s claim record. A claim-free year typically moves the policyholder to a higher discount level (or retains the maximum discount), while a year with claims moves them to a lower level (or retains zero discount).</li>
<li><strong>Markov Chain Formulation</strong>: Each discount level can be defined as a state in the Markov chain. The transitions between these states occur annually (discrete time) based on the claims experience (discrete states). The crucial assumption for the Markov property is that the future discount level depends <em>only</em> on the current discount level and claims status, not on the entire past history of claims.</li>
<li><strong>State Space Nuances</strong>: Sometimes, the definition of states might need refinement to strictly satisfy the Markov property. For instance, if a policyholder’s movement depends not just on their current discount but also on <em>how</em> they arrived there (e.g., claim history impacting future transition probabilities even if the discount level is the same), additional states might be needed. This might involve splitting a seemingly single discount level into multiple states (e.g., “30% discount with claims last year” vs. “30% discount without claims last year”).</li>
</ul></li>
<li><strong>Other Simple Applications</strong>:
<ul>
<li><strong>Actuarial Student Exam Progress</strong>: Modelling a student’s passing/failing an exam as a state, where the outcome of the next exam depends only on the previous one (pass/fail).</li>
<li><strong>Credit-worthiness Ratings</strong>: Assessing the credit rating of company debt (e.g., states A, B, D for defaulted debt), where the next year’s rating depends on the current year’s rating.</li>
<li><strong>Cumulative Claims Amount</strong>: While often modelled by continuous-time processes, the number of accidents or cumulative claims can be seen as a discrete-state, discrete-time Markov chain if observed at discrete intervals.</li>
<li><strong>Fund Performance/Investment Quartiles</strong>: Tracking a fund’s performance by classifying it into quartiles, where movement between quartiles (or staying) depends on the current quartile.</li>
<li><strong>Project Completion (e.g., Author’s Book)</strong>: Modelling the number of completed chapters of a book, where the progress in the next week depends on the current number of completed chapters.</li>
<li><strong>Company Staff Progression</strong>: Modelling employee movement between salary levels or departments within a company.</li>
<li><strong>Internet Browsing</strong>: Modelling a user’s movement between websites based on their current site.</li>
<li><strong>Health Status over Time</strong>: Recording an individual’s health status (e.g., healthy, sick, dead) at discrete time points (e.g., end of each year).</li>
<li><strong>Random Walks</strong>: Often used for modelling security prices or other economic variables, where the price at time <span class="math inline">\(t\)</span> is the previous price plus a random step. A simple random walk, where steps are +1 or -1, is a classic example of a Markov chain due to its independent increments.</li>
</ul></li>
</ul>
<hr />
</div>
<div id="describe-a-time-inhomogeneous-markov-chain-model-and-describe-simple-applications." class="section level4 hasAnchor" number="8.0.1.5">
<h4><span class="header-section-number">8.0.1.5</span> 5. Describe a time-inhomogeneous Markov chain model and describe simple applications.<a href="markov-chains.html#describe-a-time-inhomogeneous-markov-chain-model-and-describe-simple-applications." class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>While many theoretical examples focus on time-homogeneous chains for simplicity, real-world actuarial phenomena often exhibit time-inhomogeneity.</p>
<ul>
<li><strong>Definition</strong>: A Markov chain is <strong>time-inhomogeneous</strong> if its transition probabilities depend not only on the length of the time interval but also on the <em>absolute</em> times when the transition starts (<span class="math inline">\(s\)</span>) and ends (<span class="math inline">\(t\)</span>).
<ul>
<li>In contrast, a <strong>time-homogeneous</strong> Markov chain has one-step transition probabilities (<span class="math inline">\(p_{ij}\)</span>) that remain constant over time.</li>
</ul></li>
<li><strong>Chapman-Kolmogorov Equations (Time-Inhomogeneous Case)</strong>: These still apply, but the notation explicitly includes the start and end times:
<span class="math inline">\(p_{ij}(s, t) = \sum_{k \in S} p_{ik}(s, u) p_{kj}(u, t)\)</span> for all <span class="math inline">\(0 \le s \le u \le t\)</span>.</li>
<li><strong>Kolmogorov Equations</strong>: For continuous-time Markov jump processes (which can be time-inhomogeneous), the Kolmogorov forward and backward differential equations also become time-dependent, meaning the generator matrix <span class="math inline">\(A(t)\)</span> varies with time.</li>
<li><strong>Applications</strong>:
<ul>
<li><strong>Mortality and Sickness Models</strong>: The probabilities of mortality, sickness, or recovery change significantly with a person’s age. For an individual, being “healthy” at age 20 is very different from being “healthy” at age 80, in terms of future probabilities of sickness or death. Thus, modelling health status over a lifetime as a Markov chain would typically be time-inhomogeneous.</li>
<li><strong>NCD for Young Drivers</strong>: A young driver’s accident probabilities and thus NCD progression might change as they gain experience, even if the general NCD rules are fixed. Their transition probabilities would depend on their “age” or “years of experience”.</li>
<li><strong>Duration Dependence</strong>: This is a specific type of time-inhomogeneity where transition intensities depend on how long a life has been in the <em>current</em> state (duration of stay), not just their absolute age or time. This requires expanding the state space to capture this duration information to maintain the Markov property.</li>
</ul></li>
</ul>
<hr />
</div>
<div id="demonstrate-how-markov-chains-can-be-used-as-a-tool-for-modelling-and-how-they-can-be-simulated." class="section level4 hasAnchor" number="8.0.1.6">
<h4><span class="header-section-number">8.0.1.6</span> 6. Demonstrate how Markov chains can be used as a tool for modelling and how they can be simulated.<a href="markov-chains.html#demonstrate-how-markov-chains-can-be-used-as-a-tool-for-modelling-and-how-they-can-be-simulated." class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Modelling with Markov chains involves a systematic process, from defining the states to estimating parameters and simulating their behaviour.</p>
<ul>
<li><strong>Modelling Approach</strong>:
<ul>
<li><strong>Define State Space</strong>: The first critical step is to clearly define the discrete states relevant to the problem. As discussed for NCD, this might require careful thought and sometimes expanding the intuitively obvious states to ensure the Markov property holds.</li>
<li><strong>Simplicity First</strong>: Actuarial practice often advocates starting with simple models (like time-homogeneous Markov chains) and only moving to more complex ones if initial tests prove them inadequate.</li>
<li><strong>Estimate Transition Probabilities</strong>: Once the state space is fixed, the model is “fitted” by estimating the transition probabilities (<span class="math inline">\(p_{ij}\)</span>) from observed data. The maximum likelihood estimate of <span class="math inline">\(p_{ij}\)</span> is the number of observed transitions from state <span class="math inline">\(i\)</span> to state <span class="math inline">\(j\)</span> (<span class="math inline">\(n_{ij}\)</span>) divided by the total number of transitions out of state <span class="math inline">\(i\)</span> (<span class="math inline">\(n_i\)</span>). <span class="math inline">\(\hat{p}_{ij} = \frac{n_{ij}}{n_i}\)</span>.</li>
<li><strong>Test Markov Assumption (Triplets Test)</strong>: Before using a Markov chain, it’s essential to check if the Markov property is a reasonable assumption for the data. The <strong>triplets test</strong> is a common method for this. It compares observed frequencies of state sequences of length three (triplets <span class="math inline">\(i \to j \to k\)</span>) with what would be expected if the Markov property held. The test statistic is based on a chi-squared distribution.</li>
</ul></li>
<li><strong>Simulation</strong>: Simulating Markov chains is straightforward due to the Markov property; the next state only depends on the current state, simplifying the process immensely.
<ul>
<li><strong>Conceptual Method</strong>: For a finite state space, you can list the conditional distributions for each state (i.e., the rows of the transition matrix) and then sequentially draw samples to generate a path.</li>
<li><strong>R Implementation</strong>: R is a powerful tool for Markov chain modelling and simulation.
<ul>
<li><strong><code>markovchain</code> Package</strong>: This package is explicitly mentioned and recommended in the Core Reading for creating and simulating Markov chains. You can create a <code>markovchain</code> object using <code>new("markovchain", transitionMatrix = P, states = c("State1", "State2"))</code>.</li>
<li><strong>Calculating n-step Probabilities</strong>: Use <code>mc ^ n</code> (e.g., <code>mc ^ 4</code> for 4-step probabilities) on a <code>markovchain</code> object, or define a function like <code>Pn = function(P,n){...}</code> for base R matrix multiplication (<code>%*%</code>).</li>
<li><strong>Expected Distribution After n Steps</strong>: Given an initial distribution <code>d</code>, calculate <code>d * mc ^ n</code> (using <code>*</code> for markovchain objects) or <code>d %*% Pn(P, n)</code> (using <code>%*%</code> for base R).</li>
<li><strong>Long-Term Behaviour/Stationary Distribution</strong>: The <code>steadyStates(mc)</code> function directly computes the stationary distribution. As an alternative, taking a high power of the transition matrix (e.g., <code>mc ^ 40</code>) will show the convergence.</li>
<li><strong>Generating a Sample Path</strong>: The <code>rmarkovchain()</code> function from the <code>markovchain</code> package is used for this, allowing you to specify the number of transitions and starting state (e.g., <code>rmarkovchain(n = 10, object = mc, t0 = "State1")</code>).</li>
<li><strong>Fitting a Markov Chain</strong>: The <code>markovchainFit()</code> function can estimate transition probabilities from sample data (e.g., <code>fit1 = markovchainFit(past18)</code>). It provides the estimated transition matrix and log-likelihood.</li>
<li><strong>Time-Inhomogeneous Simulation</strong>: While more complex, approximate methods involve dividing time into small intervals and simulating discrete steps, whereas exact methods simulate the “jump chain” (sequence of states) and then the “holding times” (time spent in each state) separately.</li>
</ul></li>
</ul></li>
</ul>
<hr />
<p>I trust these structured notes will serve you well in your CS2 studies. Keep practicing, and remember the fundamental concepts as you work through more complex problems!</p>
</div>
</div>
</div>
<div id="practice-markov-chains" class="section level2 unnumbered hasAnchor">
<h2><code>R</code> Practice<a href="markov-chains.html#practice-markov-chains" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>TO ADD R EXAMPLE ABOUT MARKOV CHAINS HERE</strong></p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="stochastic-processes.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="markov-processes.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": true,
    "facebook": false,
    "twitter": true,
    "linkedin": true,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["twitter", "linkedin"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": "https://github.com/priyam0k/CS2/edit/main/08-markov-chains.Rmd",
    "text": "Edit"
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": null,
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "section",
    "scroll_highlight": true
  },
  "toolbar": {
    "position": "fixed"
  },
  "info": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
