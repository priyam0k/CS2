<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 15 Machine learning | Risk Modelling and Survival Analysis</title>
  <meta name="description" content="A modified version of the book on Risk Modelling and Survival Analysis, with additional notes and examples." />
  <meta name="generator" content="bookdown 0.43 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 15 Machine learning | Risk Modelling and Survival Analysis" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="A modified version of the book on Risk Modelling and Survival Analysis, with additional notes and examples." />
  <meta name="github-repo" content="priyam0k/CS2" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 15 Machine learning | Risk Modelling and Survival Analysis" />
  
  <meta name="twitter:description" content="A modified version of the book on Risk Modelling and Survival Analysis, with additional notes and examples." />
  

<meta name="author" content="Priyam" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  <link rel="shortcut icon" href="img/favicon.ico" type="image/x-icon" />
<link rel="prev" href="mortality-projection.html"/>

<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Risk Modelling and Survival Analysis</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#motivation"><i class="fa fa-check"></i>Motivation</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="r-setup.html"><a href="r-setup.html"><i class="fa fa-check"></i><b>1</b> R Setup</a>
<ul>
<li class="chapter" data-level="1.1" data-path="r-setup.html"><a href="r-setup.html#preparing-your-environment"><i class="fa fa-check"></i><b>1.1</b> Preparing your environment</a></li>
<li class="chapter" data-level="1.2" data-path="r-setup.html"><a href="r-setup.html#basic-interations-with-r"><i class="fa fa-check"></i><b>1.2</b> Basic interations with R</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="loss-distributions.html"><a href="loss-distributions.html"><i class="fa fa-check"></i><b>2</b> Loss distributions</a>
<ul>
<li class="chapter" data-level="" data-path="loss-distributions.html"><a href="loss-distributions.html#objectives-loss-distributions"><i class="fa fa-check"></i>Learning Objectives</a></li>
<li class="chapter" data-level="" data-path="loss-distributions.html"><a href="loss-distributions.html#theory-loss-distributions"><i class="fa fa-check"></i>Theory</a></li>
<li class="chapter" data-level="" data-path="loss-distributions.html"><a href="loss-distributions.html#practice-loss-distributions"><i class="fa fa-check"></i><code>R</code> Practice</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="compound-loss-distributions.html"><a href="compound-loss-distributions.html"><i class="fa fa-check"></i><b>3</b> Compound loss distributions</a>
<ul>
<li class="chapter" data-level="" data-path="compound-loss-distributions.html"><a href="compound-loss-distributions.html#objectives-compound-loss-distributions"><i class="fa fa-check"></i>Learning Objectives</a></li>
<li class="chapter" data-level="" data-path="compound-loss-distributions.html"><a href="compound-loss-distributions.html#theory-compound-loss-distributions"><i class="fa fa-check"></i>Theory</a>
<ul>
<li class="chapter" data-level="3.0.1" data-path="compound-loss-distributions.html"><a href="compound-loss-distributions.html#chapter-compound-loss-distributions"><i class="fa fa-check"></i><b>3.0.1</b> <strong>Chapter: Compound Loss Distributions</strong></a></li>
</ul></li>
<li class="chapter" data-level="" data-path="compound-loss-distributions.html"><a href="compound-loss-distributions.html#practice-compound-loss-distributions"><i class="fa fa-check"></i><code>R</code> Practice</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="copulas.html"><a href="copulas.html"><i class="fa fa-check"></i><b>4</b> Copulas</a>
<ul>
<li class="chapter" data-level="" data-path="copulas.html"><a href="copulas.html#objectives-copulas"><i class="fa fa-check"></i>Learning Objectives</a></li>
<li class="chapter" data-level="" data-path="copulas.html"><a href="copulas.html#theory-copulas"><i class="fa fa-check"></i>Theory</a>
<ul>
<li class="chapter" data-level="4.0.1" data-path="copulas.html"><a href="copulas.html#chapter-copulas-modelling-dependency-structures"><i class="fa fa-check"></i><b>4.0.1</b> <strong>Chapter: Copulas – Modelling Dependency Structures</strong></a></li>
</ul></li>
<li class="chapter" data-level="" data-path="copulas.html"><a href="copulas.html#practice-copulas"><i class="fa fa-check"></i><code>R</code> Practice</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="extreme-value-theory.html"><a href="extreme-value-theory.html"><i class="fa fa-check"></i><b>5</b> Extreme value theory</a>
<ul>
<li class="chapter" data-level="" data-path="extreme-value-theory.html"><a href="extreme-value-theory.html#objectives-evt"><i class="fa fa-check"></i>Learning Objectives</a></li>
<li class="chapter" data-level="" data-path="extreme-value-theory.html"><a href="extreme-value-theory.html#theory-evt"><i class="fa fa-check"></i>Theory</a>
<ul>
<li class="chapter" data-level="5.0.1" data-path="extreme-value-theory.html"><a href="extreme-value-theory.html#chapter-16-extreme-value-theory"><i class="fa fa-check"></i><b>5.0.1</b> Chapter 16: Extreme Value Theory</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="extreme-value-theory.html"><a href="extreme-value-theory.html#practice-evt"><i class="fa fa-check"></i><code>R</code> Practice</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="time-series.html"><a href="time-series.html"><i class="fa fa-check"></i><b>6</b> Time series</a>
<ul>
<li class="chapter" data-level="" data-path="time-series.html"><a href="time-series.html#objectives-time-series"><i class="fa fa-check"></i>Learning Objectives</a></li>
<li class="chapter" data-level="6.1" data-path="time-series.html"><a href="time-series.html#theory-time-series"><i class="fa fa-check"></i><b>6.1</b> Theory</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="time-series.html"><a href="time-series.html#time-series-a-deep-dive-for-cs2-actuarial-professionals"><i class="fa fa-check"></i><b>6.1.1</b> Time Series: A Deep Dive for CS2 Actuarial Professionals</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="time-series.html"><a href="time-series.html#practice-time-series"><i class="fa fa-check"></i><code>R</code> Practice</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="stochastic-processes.html"><a href="stochastic-processes.html"><i class="fa fa-check"></i><b>7</b> Stochastic processes</a>
<ul>
<li class="chapter" data-level="" data-path="stochastic-processes.html"><a href="stochastic-processes.html#objectives-stochastic-processes"><i class="fa fa-check"></i>Learning Objectives</a></li>
<li class="chapter" data-level="" data-path="stochastic-processes.html"><a href="stochastic-processes.html#theory-stochastic-processes"><i class="fa fa-check"></i>Theory</a>
<ul>
<li class="chapter" data-level="7.0.1" data-path="stochastic-processes.html"><a href="stochastic-processes.html#chapter-1-stochastic-processes"><i class="fa fa-check"></i><b>7.0.1</b> Chapter 1: Stochastic Processes</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="stochastic-processes.html"><a href="stochastic-processes.html#practice-stochastic-processes"><i class="fa fa-check"></i><code>R</code> Practice</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="markov-chains.html"><a href="markov-chains.html"><i class="fa fa-check"></i><b>8</b> Markov chains</a>
<ul>
<li class="chapter" data-level="" data-path="markov-chains.html"><a href="markov-chains.html#objectives-markov-chains"><i class="fa fa-check"></i>Learning Objectives</a></li>
<li class="chapter" data-level="" data-path="markov-chains.html"><a href="markov-chains.html#theory-markov-chains"><i class="fa fa-check"></i>Theory</a>
<ul>
<li class="chapter" data-level="8.0.1" data-path="markov-chains.html"><a href="markov-chains.html#chapter-markov-chains-a-comprehensive-overview"><i class="fa fa-check"></i><b>8.0.1</b> Chapter: Markov Chains – A Comprehensive Overview</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="markov-chains.html"><a href="markov-chains.html#practice-markov-chains"><i class="fa fa-check"></i><code>R</code> Practice</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="markov-processes.html"><a href="markov-processes.html"><i class="fa fa-check"></i><b>9</b> Markov processes</a>
<ul>
<li class="chapter" data-level="" data-path="markov-processes.html"><a href="markov-processes.html#objectives-09"><i class="fa fa-check"></i>Learning Objectives</a></li>
<li class="chapter" data-level="" data-path="markov-processes.html"><a href="markov-processes.html#theory-09"><i class="fa fa-check"></i>Theory</a>
<ul>
<li class="chapter" data-level="9.0.1" data-path="markov-processes.html"><a href="markov-processes.html#chapter-markov-processes-syllabus-objectives-3.3.1---3.3.8"><i class="fa fa-check"></i><b>9.0.1</b> <strong>Chapter: Markov Processes (Syllabus Objectives 3.3.1 - 3.3.8)</strong></a></li>
</ul></li>
<li class="chapter" data-level="" data-path="markov-processes.html"><a href="markov-processes.html#practice-09"><i class="fa fa-check"></i><code>R</code> Practice</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="survival-models.html"><a href="survival-models.html"><i class="fa fa-check"></i><b>10</b> Survival models</a>
<ul>
<li class="chapter" data-level="10.0.1" data-path="survival-models.html"><a href="survival-models.html#chapter-6-survival-models-the-foundation-of-life-contingencies"><i class="fa fa-check"></i><b>10.0.1</b> <strong>Chapter 6: Survival Models – The Foundation of Life Contingencies</strong></a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="lifetime-distributions.html"><a href="lifetime-distributions.html"><i class="fa fa-check"></i><b>11</b> Lifetime distributions</a>
<ul>
<li class="chapter" data-level="11.0.1" data-path="lifetime-distributions.html"><a href="lifetime-distributions.html#chapter-6-survival-models-lifetime-distributions"><i class="fa fa-check"></i><b>11.0.1</b> <strong>Chapter 6: Survival Models – Lifetime Distributions</strong></a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="transition-intensities.html"><a href="transition-intensities.html"><i class="fa fa-check"></i><b>12</b> Estimating transition intensities</a>
<ul>
<li class="chapter" data-level="12.0.1" data-path="transition-intensities.html"><a href="transition-intensities.html#chapter-9-exposed-to-risk-estimating-transition-intensities"><i class="fa fa-check"></i><b>12.0.1</b> <strong>Chapter 9: Exposed to Risk – Estimating Transition Intensities</strong></a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="graduation.html"><a href="graduation.html"><i class="fa fa-check"></i><b>13</b> Graduation</a>
<ul>
<li class="chapter" data-level="13.0.1" data-path="graduation.html"><a href="graduation.html#i.-introduction-to-graduation"><i class="fa fa-check"></i><b>13.0.1</b> <strong>I. Introduction to Graduation</strong></a></li>
<li class="chapter" data-level="13.0.2" data-path="graduation.html"><a href="graduation.html#ii.-desirable-features-of-a-graduation"><i class="fa fa-check"></i><b>13.0.2</b> <strong>II. Desirable Features of a Graduation</strong></a></li>
<li class="chapter" data-level="13.0.3" data-path="graduation.html"><a href="graduation.html#iii.-methods-of-graduation"><i class="fa fa-check"></i><b>13.0.3</b> <strong>III. Methods of Graduation</strong></a></li>
<li class="chapter" data-level="13.0.4" data-path="graduation.html"><a href="graduation.html#iv.-statistical-tests-of-graduation-adherence-to-data"><i class="fa fa-check"></i><b>13.0.4</b> <strong>IV. Statistical Tests of Graduation (Adherence to Data)</strong></a></li>
<li class="chapter" data-level="13.0.5" data-path="graduation.html"><a href="graduation.html#v.-r-implementationpractical-aspects"><i class="fa fa-check"></i><b>13.0.5</b> <strong>V. R Implementation/Practical Aspects</strong></a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="mortality-projection.html"><a href="mortality-projection.html"><i class="fa fa-check"></i><b>14</b> Mortality projection</a>
<ul>
<li class="chapter" data-level="14.1" data-path="mortality-projection.html"><a href="mortality-projection.html#mortality-projection-1"><i class="fa fa-check"></i><b>14.1</b> 14. Mortality Projection</a>
<ul>
<li class="chapter" data-level="14.1.1" data-path="mortality-projection.html"><a href="mortality-projection.html#approaches-to-forecasting-future-mortality-rates"><i class="fa fa-check"></i><b>14.1.1</b> 14.1 Approaches to Forecasting Future Mortality Rates</a></li>
<li class="chapter" data-level="14.1.2" data-path="mortality-projection.html"><a href="mortality-projection.html#sources-of-error-in-mortality-forecasts"><i class="fa fa-check"></i><b>14.1.2</b> 14.2 Sources of Error in Mortality Forecasts</a></li>
<li class="chapter" data-level="14.1.3" data-path="mortality-projection.html"><a href="mortality-projection.html#computational-aspects-r"><i class="fa fa-check"></i><b>14.1.3</b> 14.3 Computational Aspects (R)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="machine-learning.html"><a href="machine-learning.html"><i class="fa fa-check"></i><b>15</b> Machine learning</a>
<ul>
<li class="chapter" data-level="" data-path="machine-learning.html"><a href="machine-learning.html#objectives-15"><i class="fa fa-check"></i>Learning Objectives</a></li>
<li class="chapter" data-level="" data-path="machine-learning.html"><a href="machine-learning.html#theory-15"><i class="fa fa-check"></i>Theory</a></li>
<li class="chapter" data-level="15.1" data-path="machine-learning.html"><a href="machine-learning.html#chapter-machine-learning"><i class="fa fa-check"></i><b>15.1</b> Chapter: Machine Learning</a>
<ul>
<li class="chapter" data-level="15.1.1" data-path="machine-learning.html"><a href="machine-learning.html#main-branches-of-machine-learning-and-types-of-problems-addressed"><i class="fa fa-check"></i><b>15.1.1</b> 1. Main Branches of Machine Learning and Types of Problems Addressed</a></li>
<li class="chapter" data-level="15.1.2" data-path="machine-learning.html"><a href="machine-learning.html#high-level-concepts-relevant-to-learning-from-data"><i class="fa fa-check"></i><b>15.1.2</b> 2. High-Level Concepts Relevant to Learning from Data</a></li>
<li class="chapter" data-level="15.1.3" data-path="machine-learning.html"><a href="machine-learning.html#key-supervised-and-unsupervised-machine-learning-techniques-regression-vs.-classification-generative-vs.-discriminative-models"><i class="fa fa-check"></i><b>15.1.3</b> 3. Key Supervised and Unsupervised Machine Learning Techniques: Regression vs. Classification, Generative vs. Discriminative Models</a></li>
<li class="chapter" data-level="15.1.4" data-path="machine-learning.html"><a href="machine-learning.html#application-of-machine-learning-techniques-e.g.-penalised-regression-decision-trees-using-appropriate-software"><i class="fa fa-check"></i><b>15.1.4</b> 4. Application of Machine Learning Techniques (e.g., Penalised Regression, Decision Trees) using Appropriate Software</a></li>
<li class="chapter" data-level="15.1.5" data-path="machine-learning.html"><a href="machine-learning.html#perspectives-of-statisticians-data-scientists-and-other-quantitative-researchers"><i class="fa fa-check"></i><b>15.1.5</b> 5. Perspectives of Statisticians, Data Scientists, and Other Quantitative Researchers</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="machine-learning.html"><a href="machine-learning.html#practice-15"><i class="fa fa-check"></i><code>R</code> Practice</a></li>
</ul></li>
<li class="divider"></li>
<li>Adapted by <a href="https://github.com/priyam0k">Priyam</a> from the original by <a href="https://github.com/agarbiak" target="blank">Alex Garbiak</a>.</li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Risk Modelling and Survival Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="machine-learning" class="section level1 hasAnchor" number="15">
<h1><span class="header-section-number">Chapter 15</span> Machine learning<a href="machine-learning.html#machine-learning" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="objectives-15" class="section level2 unnumbered hasAnchor">
<h2>Learning Objectives<a href="machine-learning.html#objectives-15" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ol style="list-style-type: decimal">
<li>Explain the main branches of machine learning and describe examples of the types of problems typically addressed by machine learning.</li>
<li>Explain and apply high-level concepts relevant to learning from data.</li>
<li>Describe and give examples of key supervised and unsupervised machine learning techniques, explaining the difference between regression and classification and between generative and discriminative models.</li>
<li>Explain in detail and use appropriate software to apply machine learning techniques (e.g. penalised regression and decision trees) to simple problems.</li>
<li>Demonstrate an understanding of the perspectives of statisticians, data scientists, and other quantitative researchers from non-actuarial backgrounds.</li>
</ol>
</div>
<div id="theory-15" class="section level2 unnumbered hasAnchor">
<h2>Theory<a href="machine-learning.html#theory-15" class="anchor-section" aria-label="Anchor link to header"></a></h2>
</div>
<div id="chapter-machine-learning" class="section level2 hasAnchor" number="15.1">
<h2><span class="header-section-number">15.1</span> Chapter: Machine Learning<a href="machine-learning.html#chapter-machine-learning" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Machine learning (ML) describes a set of methods by which computer algorithms are developed and applied to data to detect and exploit patterns, primarily focusing on making predictions rather than understanding the statistical properties of a population. The field’s recent surge in importance is largely due to the increased availability of “big data” and a rapid rise in computing power. For machine learning to be a useful approach, three conditions must generally apply: a pattern should exist in the data, this pattern cannot be practically defined mathematically by traditional methods, and relevant data must be available.</p>
<div id="main-branches-of-machine-learning-and-types-of-problems-addressed" class="section level3 hasAnchor" number="15.1.1">
<h3><span class="header-section-number">15.1.1</span> 1. Main Branches of Machine Learning and Types of Problems Addressed<a href="machine-learning.html#main-branches-of-machine-learning-and-types-of-problems-addressed" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Machine learning techniques are broadly categorised into several branches:</p>
<ul>
<li><strong>Supervised Learning:</strong> Associated with predictive models where the output is specified. The algorithm is given a specific target to aim at, which can be numerical or categorical, and it attempts to converge on parameters that provide the best prediction.
<ul>
<li><strong>Examples of Problems:</strong>
<ul>
<li>Predicting whether a person will default on a loan.</li>
<li>Classifying the risk for motor insurance policyholders using in-car monitoring devices.</li>
<li>Identifying marker genes associated with particular medical conditions.</li>
<li>Handwriting recognition (turning handwriting into computer text).</li>
</ul></li>
</ul></li>
<li><strong>Unsupervised Learning:</strong> Operates without a specified target outcome. The algorithm aims to identify patterns or information within unlabelled data.
<ul>
<li><strong>Examples of Problems:</strong>
<ul>
<li>Identifying clusters within data, such as grouping car insurance policyholders based on geographical area, premium paid, and claims experience to form homogeneous groups.</li>
<li>Market basket analysis (identifying items commonly purchased together from retail transactions).</li>
<li>Text analysis (e.g., analysing common word combinations for predictive text suggestions).</li>
</ul></li>
</ul></li>
<li><strong>Semi-supervised Learning:</strong> Involves using a mixture of supervised and unsupervised learning. For instance, cluster analysis could identify groups, which are then labelled with a variable, followed by a supervised classification algorithm to predict class membership.</li>
<li><strong>Reinforcement Learning:</strong> The learner is not given a target output directly but uses input data to choose an output. It receives feedback (a “reward”) on how well it performed and adjusts its actions to maximise the total reward through trial and error.</li>
</ul>
<p><strong>General Examples of Problems Addressed by Machine Learning:</strong>
Beyond actuarial applications, machine learning solves various real-world problems:
* Targeting advertising at consumers using websites.
* Locating stock within supermarkets to maximise turnover.
* Forecasting election results.
* Spam filtering (identifying and removing unwanted emails).
* Face recognition to identify known criminals.
* Recommending items to purchase on online shopping sites.
* Recognising voice commands (e.g., Siri, Cortana, Alexa).</p>
</div>
<div id="high-level-concepts-relevant-to-learning-from-data" class="section level3 hasAnchor" number="15.1.2">
<h3><span class="header-section-number">15.1.2</span> 2. High-Level Concepts Relevant to Learning from Data<a href="machine-learning.html#high-level-concepts-relevant-to-learning-from-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The process of learning from data in machine learning involves several key high-level concepts:</p>
<ul>
<li><strong>Target Function and Hypothesis:</strong> There is an underlying, often unknown, “target function” (<span class="math inline">\(f\)</span>) that maps a set of input variables (or “features”) to an output (<span class="math inline">\(y\)</span>). The goal of machine learning is to develop a “hypothesis” (<span class="math inline">\(g\)</span>) that approximates this unknown target function.</li>
<li><strong>Data Collection and Preparation:</strong> Before model building, data must be assembled in a computer-readable format (e.g., rectangular format with one row per case and one column per variable). This stage involves cleaning data, replacing missing values, and checking for errors. While some Exploratory Data Analysis (EDA) is done, extensive EDA might be avoided to prevent influencing model choice.</li>
<li><strong>Feature Scaling:</strong> For certain machine learning techniques, it is crucial that input variables are on a similar scale for effective operation.</li>
<li><strong>Splitting Data (Training, Validation, Testing):</strong>
<ul>
<li><strong>Training Data:</strong> Used to fit and train the model, where the algorithm attempts to predict known outcomes.</li>
<li><strong>Validation Data:</strong> A subset used to evaluate the model’s performance on unseen data and, critically, to set “hyperparameters”.</li>
<li><strong>Test Data:</strong> A final, withheld portion of the data, not used during training or validation, to provide an unbiased assessment of the model’s performance on out-of-sample data. A common split is 60% training, 20% validation, 20% testing.</li>
</ul></li>
<li><strong>Parameters vs. Hyperparameters:</strong>
<ul>
<li><strong>Parameters:</strong> Internal to the model; their values are estimated directly from the data during training and are integral to making predictions. For example, the coefficients (weights) in a regression model.</li>
<li><strong>Hyperparameters:</strong> Higher-level attributes of the model that cannot be estimated from the data. They are typically set by the practitioner (e.g., using heuristic guidelines) and are crucial for the model’s predictive success. Examples include the number of covariates to include, the number of categories in a classification, or the “learning rate”, or the number of clusters in a K-means algorithm.</li>
</ul></li>
<li><strong>Overfitting and Regularisation:</strong>
<ul>
<li><strong>Overfitting:</strong> A significant danger where a model, especially one with many parameters or features, learns the “noise” or idiosyncratic characteristics of the training data too well, failing to generalise effectively to new, unseen data. This leads to the identification of patterns that are specific to the training data and do not generalise.</li>
<li><strong>Regularisation (Penalisation):</strong> A technique to reduce overfitting in highly parameterised models. It involves adding a “cost for model complexity” (a penalty function) to the loss function that the model seeks to minimise. This encourages the model to produce parameter estimates closer to a set of target values or to simplify the model, balancing adherence to data with a desired level of smoothness. Common types include Ridge regression (<span class="math inline">\(\sum w_j^2\)</span>) and Lasso regression (<span class="math inline">\(\sum |w_j|\)</span>), where <span class="math inline">\(w_j\)</span> are weights.</li>
</ul></li>
<li><strong>Bias-Variance Trade-off:</strong> The expected mean squared error (MSE) of a model’s prediction on unseen data can be decomposed into three components: noise (irreducible error), bias, and variance.
<ul>
<li><strong>Bias:</strong> Error due to simplistic assumptions or the model’s inability to capture the true underlying relationship. A high-bias model might “underfit” the data.</li>
<li><strong>Variance:</strong> Error due to the model’s sensitivity to fluctuations in the training data. A high-variance model might “overfit” the data.
ML aims to find a balance in this trade-off, as reducing bias often increases variance, and vice versa.</li>
</ul></li>
<li><strong>Reproducibility:</strong> It is essential that any data analysis, including machine learning, is reproducible. This means another researcher can take the same data, follow the described analysis steps, and obtain the same results. Key aspects include fully described and available data, clear documentation of data modifications (feature engineering) and algorithm choices (including parameters and their selection). For stochastic models, using <code>set.seed()</code> in R ensures the reproducibility of random elements.</li>
</ul>
</div>
<div id="key-supervised-and-unsupervised-machine-learning-techniques-regression-vs.-classification-generative-vs.-discriminative-models" class="section level3 hasAnchor" number="15.1.3">
<h3><span class="header-section-number">15.1.3</span> 3. Key Supervised and Unsupervised Machine Learning Techniques: Regression vs. Classification, Generative vs. Discriminative Models<a href="machine-learning.html#key-supervised-and-unsupervised-machine-learning-techniques-regression-vs.-classification-generative-vs.-discriminative-models" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Machine learning techniques are often distinguished by the nature of their output and the way they model relationships:</p>
<div id="supervised-learning-techniques" class="section level4 hasAnchor" number="15.1.3.1">
<h4><span class="header-section-number">15.1.3.1</span> Supervised Learning Techniques:<a href="machine-learning.html#supervised-learning-techniques" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><strong>Decision Trees (CART - Classification and Regression Trees):</strong>
<ul>
<li>A predictive model that partitions data into subsets based on input variables.</li>
<li>Represented as a binary tree diagram, where a series of questions are asked to split the data until a “leaf” node is reached, which provides the prediction.</li>
<li><strong>Regression Trees:</strong> Output is a continuous numerical value (e.g., the mean value of the target variable in a node). The splitting criteria aim to minimise the squared error cost function.</li>
<li><strong>Classification Trees:</strong> Output is a categorical variable (e.g., the most frequent class in a node). The splitting criteria often use measures like the Gini index.</li>
<li>Construction commonly uses “greedy splitting,” where the best split is chosen at each stage to maximise the immediate reduction in the loss function. Overfitting can be mitigated by stopping criteria or pruning.</li>
</ul></li>
<li><strong>Bagged Decision Trees (Bagging):</strong> Reduces variance by averaging predictions from multiple decision trees. Each tree is trained on a different “bootstrap” sample (random sub-samples with replacement) of the original data.</li>
<li><strong>Random Forests:</strong> An improvement over bagging. In addition to training on bootstrap samples, at each split, only a random subset of features is considered. This further reduces correlation between trees, often leading to better performance (e.g., reduced Mean Squared Error).</li>
<li><strong>Naïve Bayes Classification:</strong>
<ul>
<li>A probabilistic classifier based on Bayes’ theorem.</li>
<li>The “naïve” assumption is that input variables are independent of each other, conditional on the output variable. This simplifies the calculation of the posterior probability <span class="math inline">\(P(y|x_1, \dots, x_J) \propto P(y) \prod_{j=1}^J P(x_j|y)\)</span>.</li>
<li>Used for classification problems. It can produce surprisingly good results despite its strong independence assumption.</li>
</ul></li>
<li><strong>Penalised Regression (e.g., Penalised Generalised Linear Models):</strong> These extend traditional regression models by adding a penalty term to the likelihood function. This helps prevent overfitting and encourages simpler models, making them more robust to noise in the training data.</li>
</ul>
</div>
<div id="unsupervised-learning-techniques" class="section level4 hasAnchor" number="15.1.3.2">
<h4><span class="header-section-number">15.1.3.2</span> Unsupervised Learning Techniques:<a href="machine-learning.html#unsupervised-learning-techniques" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><strong>K-means Clustering:</strong>
<ul>
<li>Partitions data into <em>K</em> clusters, aiming to minimise the within-cluster sum of squares.</li>
<li>It’s an iterative algorithm: initial cluster centres are chosen (randomly or with prior knowledge), each data point is assigned to its nearest centre, new centroids are calculated, and points are re-assigned until convergence.</li>
<li>The “elbow method” (plotting total within-groups sum of squares against <em>K</em>) can help determine an appropriate number of clusters.</li>
<li>Advantages: simple, flexible, efficient. Disadvantages: not guaranteed to find the optimal set of clusters, requires a reasonable initial guess for <em>K</em>.</li>
</ul></li>
<li><strong>Principal Component Analysis (PCA):</strong>
<ul>
<li>A dimensionality reduction technique that transforms data into a new set of uncorrelated variables (principal components).</li>
<li>The principal components capture most of the variance in the original data, simplifying high-dimensional datasets and reducing noise.</li>
</ul></li>
</ul>
</div>
<div id="regression-vs.-classification" class="section level4 hasAnchor" number="15.1.3.3">
<h4><span class="header-section-number">15.1.3.3</span> Regression vs. Classification:<a href="machine-learning.html#regression-vs.-classification" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><strong>Regression Problem:</strong> Involves predicting a <strong>numerical</strong> (continuous) output value.
<ul>
<li><em>Example:</em> Predicting a university graduate’s salary at age 40 based on their subject, degree grade, and sex.</li>
</ul></li>
<li><strong>Classification Problem:</strong> Involves predicting which <strong>category</strong> a case falls into (discrete or categorical output).
<ul>
<li><em>Example:</em> Grouping car insurance policyholders based on geographical area, premium paid, and claims experience.</li>
</ul></li>
</ul>
</div>
<div id="generative-vs.-discriminative-models" class="section level4 hasAnchor" number="15.1.3.4">
<h4><span class="header-section-number">15.1.3.4</span> Generative vs. Discriminative Models:<a href="machine-learning.html#generative-vs.-discriminative-models" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>This distinction relates to how models approach the task of classification:</p>
<ul>
<li><strong>Generative Models:</strong> Model the <strong>joint probability distribution</strong> of the input features (<span class="math inline">\(\boldsymbol{X}\)</span>) and the output classes (<span class="math inline">\(\boldsymbol{Y}\)</span>), i.e., <span class="math inline">\(P(\boldsymbol{X}, \boldsymbol{Y})\)</span>. They then use Bayes’ theorem to derive the conditional probability <span class="math inline">\(P(\boldsymbol{Y}|\boldsymbol{X})\)</span>, which is used for classification.
<ul>
<li><em>Example:</em> Naïve Bayes classifiers are generative.</li>
</ul></li>
<li><strong>Discriminative Models:</strong> Directly model the <strong>conditional probability distribution</strong> of the output classes given the input features, i.e., <span class="math inline">\(P(\boldsymbol{Y}|\boldsymbol{X})\)</span>, without explicitly modelling the joint distribution.
<ul>
<li><em>Examples:</em> Logistic regression and Cox proportional hazards models are discriminative.</li>
</ul></li>
</ul>
</div>
</div>
<div id="application-of-machine-learning-techniques-e.g.-penalised-regression-decision-trees-using-appropriate-software" class="section level3 hasAnchor" number="15.1.4">
<h3><span class="header-section-number">15.1.4</span> 4. Application of Machine Learning Techniques (e.g., Penalised Regression, Decision Trees) using Appropriate Software<a href="machine-learning.html#application-of-machine-learning-techniques-e.g.-penalised-regression-decision-trees-using-appropriate-software" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The CS2 syllabus emphasises the use of appropriate computer packages, particularly R, for applying mortality forecasting models and, by extension, machine learning techniques.</p>
<div id="penalised-regression-in-r" class="section level4 hasAnchor" number="15.1.4.1">
<h4><span class="header-section-number">15.1.4.1</span> Penalised Regression in R:<a href="machine-learning.html#penalised-regression-in-r" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>As noted earlier, penalised regression methods (like Ridge and Lasso) incorporate a penalty into the likelihood function to improve model stability and generalisation.</li>
<li>In R, these can be implemented by defining a negative log-likelihood function that includes the penalty term, and then using optimisation functions (e.g., <code>nlm()</code> for non-linear minimisation) to find the parameters that minimise this adjusted function.</li>
<li><em>Example:</em> If a maximum likelihood estimate (MLE) of a parameter is <span class="math inline">\(\hat{\mu}\)</span>, penalised regression will influence <span class="math inline">\(\hat{\mu}\)</span> towards a predefined target value, with the strength of this influence determined by the regularisation parameter.</li>
</ul>
</div>
<div id="decision-trees-and-ensemble-methods-in-r" class="section level4 hasAnchor" number="15.1.4.2">
<h4><span class="header-section-number">15.1.4.2</span> Decision Trees (and ensemble methods) in R:<a href="machine-learning.html#decision-trees-and-ensemble-methods-in-r" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><strong>Decision Trees:</strong> The <code>tree</code> package in R can be used to construct decision trees.
<ul>
<li>You can build a tree to predict a categorical (e.g., flower species) or numerical output based on input features.</li>
<li><code>plot()</code> and <code>text()</code> functions are used to visualise the tree structure.</li>
<li>To evaluate, you can predict species for a test dataset and construct a confusion matrix using <code>table()</code> to compare predicted vs. actual values.</li>
</ul></li>
<li><strong>Bagged Decision Trees and Random Forests:</strong> The <code>randomForest</code> package in R is used for these ensemble methods.
<ul>
<li><code>randomForest()</code> function is used to build these models, specifying the number of trees (<code>ntree</code>) and the number of input variables to consider at each split (<code>mtry</code>).</li>
<li>These models aim to improve predictive performance and reduce variance compared to single decision trees.</li>
<li><em>Example:</em> Using the <code>randomForest</code> package to predict survival on the Titanic dataset, comparing actual vs. predicted outcomes using a confusion matrix.</li>
</ul></li>
<li><strong>Naïve Bayes Classification in R:</strong>
<ul>
<li>While there are specific packages, the concept can be implemented using custom R functions to calculate the required conditional probabilities based on the “naïve” independence assumption.</li>
<li><em>Example:</em> Calculating the probability of a whisky being of a certain type given its characteristics (smoky, fruity, colour) using these probabilities.</li>
</ul></li>
<li><strong>K-means Clustering in R:</strong>
<ul>
<li>The <code>kmeans()</code> function in R is used to perform K-means clustering.</li>
<li>You can specify the number of clusters (<code>K</code>) or investigate different values of <code>K</code> by plotting the “total within-groups sum of squares” (elbow method).</li>
<li>The output provides cluster assignments and cluster centres, which can be analysed and plotted to describe the resulting groups.</li>
</ul></li>
</ul>
</div>
</div>
<div id="perspectives-of-statisticians-data-scientists-and-other-quantitative-researchers" class="section level3 hasAnchor" number="15.1.5">
<h3><span class="header-section-number">15.1.5</span> 5. Perspectives of Statisticians, Data Scientists, and Other Quantitative Researchers<a href="machine-learning.html#perspectives-of-statisticians-data-scientists-and-other-quantitative-researchers" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Machine learning involves applying methods to data to solve real-world problems, an area that often overlaps with the work of other quantitative researchers, such as statisticians and data scientists.</p>
<ul>
<li><strong>Overlap in Work:</strong> Statisticians routinely engage in activities like data mining, data reduction (e.g., Principal Component Analysis), and estimating models like logistic regression, which are also machine learning techniques.</li>
<li><strong>Differences in Terminology:</strong> There are straightforward differences in terminology. For instance, machine learning practitioners talk about “training” a model or hyperparameters, while statisticians might refer to “fitting” a model or “choosing” higher-level parameters. These are essentially different words for the same underlying activities.</li>
<li><strong>Differences in Aims and Emphasis:</strong>
<ul>
<li><strong>Machine Learning Focus:</strong> Often more concerned with <strong>predictive accuracy</strong> for new, unseen data. The primary goal is to find an algorithm that can predict outcomes well.</li>
<li><strong>Traditional Statistics Focus (arguably):</strong> Historically, traditional statistics has often been more concerned with <strong>inference, understanding the underlying statistical properties</strong> of a population, and the significance and size of the fitted parameters (i.e., how much effect parameters have on the final outcome).</li>
</ul></li>
<li><strong>Model Utility and Acceptance:</strong> Regardless of the disciplinary background, models are generally more useful if they are easy to explain and are acceptable to regulators or other supervisory bodies. This includes avoiding classifications or discriminations that are perceived as undesirable (e.g., those based on gender).</li>
</ul>
<hr />
</div>
</div>
<div id="practice-15" class="section level2 unnumbered hasAnchor">
<h2><code>R</code> Practice<a href="machine-learning.html#practice-15" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We are managing a <code>portfolio</code> of investments that contains 200 assets. In this <code>portfolio</code> we measure the following features:</p>
<ul>
<li>Price-to-Earnings Ratio (“<em>PE</em>”), labelled <span class="math inline">\(x1\)</span> with <span class="math inline">\(x1 \sim \mathcal{N}(3,\,1)\)</span></li>
<li>Price-to-Book Ratio (“<em>PB</em>”), labelled <span class="math inline">\(x2\)</span> with 65% of the assets following <span class="math inline">\(\mathcal{N}(10,\,1)\)</span> and the remaining 35% following <span class="math inline">\(\mathcal{N}(4,\,1)\)</span></li>
</ul>
<p>We replicate this in <code>R</code> as follows:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="machine-learning.html#cb1-1" tabindex="-1"></a><span class="fu">library</span>(dplyr) <span class="co"># Data manipulation</span></span>
<span id="cb1-2"><a href="machine-learning.html#cb1-2" tabindex="-1"></a></span>
<span id="cb1-3"><a href="machine-learning.html#cb1-3" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>) <span class="co"># Fix result</span></span>
<span id="cb1-4"><a href="machine-learning.html#cb1-4" tabindex="-1"></a>portfolio <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb1-5"><a href="machine-learning.html#cb1-5" tabindex="-1"></a>  <span class="at">x1 =</span> <span class="fu">rnorm</span>(<span class="dv">200</span>, <span class="dv">3</span>, <span class="dv">1</span>),</span>
<span id="cb1-6"><a href="machine-learning.html#cb1-6" tabindex="-1"></a>  <span class="at">x2 =</span> <span class="fu">scale</span>(</span>
<span id="cb1-7"><a href="machine-learning.html#cb1-7" tabindex="-1"></a>    <span class="fu">c</span>(</span>
<span id="cb1-8"><a href="machine-learning.html#cb1-8" tabindex="-1"></a>      <span class="fu">rnorm</span>(<span class="dv">70</span>, <span class="dv">4</span>, <span class="dv">1</span>),</span>
<span id="cb1-9"><a href="machine-learning.html#cb1-9" tabindex="-1"></a>      <span class="fu">rnorm</span>(<span class="dv">130</span>, <span class="dv">10</span>, <span class="dv">1</span>)</span>
<span id="cb1-10"><a href="machine-learning.html#cb1-10" tabindex="-1"></a>    )</span>
<span id="cb1-11"><a href="machine-learning.html#cb1-11" tabindex="-1"></a>  )</span>
<span id="cb1-12"><a href="machine-learning.html#cb1-12" tabindex="-1"></a>)</span>
<span id="cb1-13"><a href="machine-learning.html#cb1-13" tabindex="-1"></a></span>
<span id="cb1-14"><a href="machine-learning.html#cb1-14" tabindex="-1"></a><span class="fu">glimpse</span>(portfolio)</span></code></pre></div>
<pre><code>## Rows: 200
## Columns: 2
## $ x1 &lt;dbl&gt; 4.3709584, 2.4353018, 3.3631284, 3.6328626, 3.4042683, 2.8938755, 4…
## $ x2 &lt;dbl&gt; -1.9258826, -1.1653603, -0.8925320, -0.6031994, -1.7225948, -1.6489…</code></pre>
<p>Here the <code class="sourceCode r"><span class="fu">scale</span>()</code> function scales each element in the result by subtracting the sample mean and dividing by the sample standard deviation.</p>
<p>Next we want to explore whether these 200 assets can be divided into two clusters which we will label arbitrarily <code>A</code> and <code>B</code> based on the two metrics we have measured, PE (as <span class="math inline">\(x1\)</span>) and PB (as <span class="math inline">\(x2\)</span>).</p>
<p>We will first assign the assets evenly into these two clusters:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="machine-learning.html#cb3-1" tabindex="-1"></a>group_label_stage1 <span class="ot">&lt;-</span> <span class="fu">c</span>(</span>
<span id="cb3-2"><a href="machine-learning.html#cb3-2" tabindex="-1"></a>  <span class="fu">rep</span>(<span class="st">&quot;A&quot;</span>, <span class="dv">100</span>),</span>
<span id="cb3-3"><a href="machine-learning.html#cb3-3" tabindex="-1"></a>  <span class="fu">rep</span>(<span class="st">&quot;B&quot;</span>, <span class="dv">100</span>)</span>
<span id="cb3-4"><a href="machine-learning.html#cb3-4" tabindex="-1"></a>)</span>
<span id="cb3-5"><a href="machine-learning.html#cb3-5" tabindex="-1"></a></span>
<span id="cb3-6"><a href="machine-learning.html#cb3-6" tabindex="-1"></a>portfolio <span class="ot">&lt;-</span> portfolio <span class="sc">%&gt;%</span> </span>
<span id="cb3-7"><a href="machine-learning.html#cb3-7" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">group_label_stage1 =</span> group_label_stage1)</span>
<span id="cb3-8"><a href="machine-learning.html#cb3-8" tabindex="-1"></a></span>
<span id="cb3-9"><a href="machine-learning.html#cb3-9" tabindex="-1"></a>ClusterACentre <span class="ot">&lt;-</span> <span class="fu">c</span>(</span>
<span id="cb3-10"><a href="machine-learning.html#cb3-10" tabindex="-1"></a>  <span class="fu">mean</span>(portfolio<span class="sc">$</span>x1[portfolio<span class="sc">$</span>group_label_stage1 <span class="sc">==</span> <span class="st">&quot;A&quot;</span>]),</span>
<span id="cb3-11"><a href="machine-learning.html#cb3-11" tabindex="-1"></a>  <span class="fu">mean</span>(portfolio<span class="sc">$</span>x2[portfolio<span class="sc">$</span>group_label_stage1 <span class="sc">==</span> <span class="st">&quot;A&quot;</span>])</span>
<span id="cb3-12"><a href="machine-learning.html#cb3-12" tabindex="-1"></a>)</span>
<span id="cb3-13"><a href="machine-learning.html#cb3-13" tabindex="-1"></a></span>
<span id="cb3-14"><a href="machine-learning.html#cb3-14" tabindex="-1"></a>ClusterBCentre <span class="ot">&lt;-</span> <span class="fu">c</span>(</span>
<span id="cb3-15"><a href="machine-learning.html#cb3-15" tabindex="-1"></a>  <span class="fu">mean</span>(portfolio<span class="sc">$</span>x1[portfolio<span class="sc">$</span>group_label_stage1 <span class="sc">==</span> <span class="st">&quot;B&quot;</span>]),</span>
<span id="cb3-16"><a href="machine-learning.html#cb3-16" tabindex="-1"></a>  <span class="fu">mean</span>(portfolio<span class="sc">$</span>x2[portfolio<span class="sc">$</span>group_label_stage1 <span class="sc">==</span> <span class="st">&quot;B&quot;</span>])</span>
<span id="cb3-17"><a href="machine-learning.html#cb3-17" tabindex="-1"></a>)</span>
<span id="cb3-18"><a href="machine-learning.html#cb3-18" tabindex="-1"></a></span>
<span id="cb3-19"><a href="machine-learning.html#cb3-19" tabindex="-1"></a><span class="fu">glimpse</span>(portfolio)</span></code></pre></div>
<pre><code>## Rows: 200
## Columns: 3
## $ x1                 &lt;dbl&gt; 4.3709584, 2.4353018, 3.3631284, 3.6328626, 3.40426…
## $ x2                 &lt;dbl&gt; -1.9258826, -1.1653603, -0.8925320, -0.6031994, -1.…
## $ group_label_stage1 &lt;chr&gt; &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;…</code></pre>
<p>We have:</p>
<ul>
<li>The centre of cluster <code>A</code>, given by <span class="math inline">\((x1_A,\, x2_A)\)</span> is 3.033, -0.691, and</li>
<li>The centre of cluster <code>B</code>, given by <span class="math inline">\((x1_A,\, x2_A)\)</span> is 2.913, 0.691.</li>
</ul>
<p>Next we want to calculate the Euclidean distance between:</p>
<ul>
<li><span class="math inline">\((x1, x2)\)</span> and the centre of cluster <code>A</code>, and</li>
<li><span class="math inline">\((x1, x2)\)</span> and the centre of cluster <code>B</code>.</li>
</ul>
<p>We will label these distances as <code>dist_A</code> and <code>dist_B</code> respectively.</p>
<p>The Euclidean distance is defined as:</p>
<ul>
<li>For <code>dist_A</code>: <span class="math inline">\(\sqrt{(x1-x1_A)^2+(x2-x2_A)^2}\)</span>, and</li>
<li>For <code>dist_B</code>: <span class="math inline">\(\sqrt{(x1-x1_B)^2+(x2-x2_B)^2}\)</span>.</li>
</ul>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="machine-learning.html#cb5-1" tabindex="-1"></a>dist_A <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(</span>
<span id="cb5-2"><a href="machine-learning.html#cb5-2" tabindex="-1"></a>  (portfolio<span class="sc">$</span>x1 <span class="sc">-</span> ClusterACentre[<span class="dv">1</span>])<span class="sc">^</span><span class="dv">2</span> </span>
<span id="cb5-3"><a href="machine-learning.html#cb5-3" tabindex="-1"></a>  <span class="sc">+</span> (portfolio<span class="sc">$</span>x2 <span class="sc">-</span> ClusterACentre[<span class="dv">2</span>])<span class="sc">^</span><span class="dv">2</span> </span>
<span id="cb5-4"><a href="machine-learning.html#cb5-4" tabindex="-1"></a>)</span>
<span id="cb5-5"><a href="machine-learning.html#cb5-5" tabindex="-1"></a></span>
<span id="cb5-6"><a href="machine-learning.html#cb5-6" tabindex="-1"></a>dist_B <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(</span>
<span id="cb5-7"><a href="machine-learning.html#cb5-7" tabindex="-1"></a>  (portfolio<span class="sc">$</span>x1 <span class="sc">-</span> ClusterBCentre[<span class="dv">1</span>])<span class="sc">^</span><span class="dv">2</span> </span>
<span id="cb5-8"><a href="machine-learning.html#cb5-8" tabindex="-1"></a>  <span class="sc">+</span> (portfolio<span class="sc">$</span>x2 <span class="sc">-</span> ClusterBCentre[<span class="dv">2</span>])<span class="sc">^</span><span class="dv">2</span> </span>
<span id="cb5-9"><a href="machine-learning.html#cb5-9" tabindex="-1"></a>)</span>
<span id="cb5-10"><a href="machine-learning.html#cb5-10" tabindex="-1"></a></span>
<span id="cb5-11"><a href="machine-learning.html#cb5-11" tabindex="-1"></a>portfolio <span class="ot">&lt;-</span> portfolio <span class="sc">%&gt;%</span> </span>
<span id="cb5-12"><a href="machine-learning.html#cb5-12" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb5-13"><a href="machine-learning.html#cb5-13" tabindex="-1"></a>    <span class="at">dist_A =</span> dist_A,</span>
<span id="cb5-14"><a href="machine-learning.html#cb5-14" tabindex="-1"></a>    <span class="at">dist_B =</span> dist_B</span>
<span id="cb5-15"><a href="machine-learning.html#cb5-15" tabindex="-1"></a>  )</span>
<span id="cb5-16"><a href="machine-learning.html#cb5-16" tabindex="-1"></a></span>
<span id="cb5-17"><a href="machine-learning.html#cb5-17" tabindex="-1"></a><span class="fu">glimpse</span>(portfolio)</span></code></pre></div>
<pre><code>## Rows: 200
## Columns: 5
## $ x1                 &lt;dbl&gt; 4.3709584, 2.4353018, 3.3631284, 3.6328626, 3.40426…
## $ x2                 &lt;dbl&gt; -1.9258826, -1.1653603, -0.8925320, -0.6031994, -1.…
## $ group_label_stage1 &lt;chr&gt; &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;…
## $ dist_A             &lt;dbl&gt; 1.8210075, 0.7626050, 0.3871328, 0.6067517, 1.09642…
## $ dist_B             &lt;dbl&gt; 2.995957, 1.916835, 1.646514, 1.481271, 2.463299, 2…</code></pre>
<p>Now we will update the cluster labels (<code>A</code> and <code>B</code>) by assigning to each asset the label of the cluster whose centre is nearest from <code>dist_A</code> and <code>dist_B</code>.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="machine-learning.html#cb7-1" tabindex="-1"></a>portfolio <span class="ot">&lt;-</span> portfolio <span class="sc">%&gt;%</span> </span>
<span id="cb7-2"><a href="machine-learning.html#cb7-2" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb7-3"><a href="machine-learning.html#cb7-3" tabindex="-1"></a>    <span class="at">group_label_stage2 =</span> <span class="fu">ifelse</span>(portfolio<span class="sc">$</span>dist_A <span class="sc">&lt;=</span> portfolio<span class="sc">$</span>dist_B, <span class="st">&quot;A&quot;</span>, <span class="st">&quot;B&quot;</span>)</span>
<span id="cb7-4"><a href="machine-learning.html#cb7-4" tabindex="-1"></a>  )</span>
<span id="cb7-5"><a href="machine-learning.html#cb7-5" tabindex="-1"></a></span>
<span id="cb7-6"><a href="machine-learning.html#cb7-6" tabindex="-1"></a><span class="fu">glimpse</span>(portfolio)</span></code></pre></div>
<pre><code>## Rows: 200
## Columns: 6
## $ x1                 &lt;dbl&gt; 4.3709584, 2.4353018, 3.3631284, 3.6328626, 3.40426…
## $ x2                 &lt;dbl&gt; -1.9258826, -1.1653603, -0.8925320, -0.6031994, -1.…
## $ group_label_stage1 &lt;chr&gt; &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;…
## $ dist_A             &lt;dbl&gt; 1.8210075, 0.7626050, 0.3871328, 0.6067517, 1.09642…
## $ dist_B             &lt;dbl&gt; 2.995957, 1.916835, 1.646514, 1.481271, 2.463299, 2…
## $ group_label_stage2 &lt;chr&gt; &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;…</code></pre>
<p>Let’s generate a 2x2 matrix showing the number of assets with each possible combination of values from <code>group_label_stage1</code> and <code>group_label_stage2</code>:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="machine-learning.html#cb9-1" tabindex="-1"></a>combos <span class="ot">&lt;-</span> portfolio <span class="sc">%&gt;%</span> </span>
<span id="cb9-2"><a href="machine-learning.html#cb9-2" tabindex="-1"></a>  <span class="fu">count</span>(group_label_stage1, group_label_stage2)</span>
<span id="cb9-3"><a href="machine-learning.html#cb9-3" tabindex="-1"></a></span>
<span id="cb9-4"><a href="machine-learning.html#cb9-4" tabindex="-1"></a><span class="fu">matrix</span>(</span>
<span id="cb9-5"><a href="machine-learning.html#cb9-5" tabindex="-1"></a>  combos<span class="sc">$</span>n,</span>
<span id="cb9-6"><a href="machine-learning.html#cb9-6" tabindex="-1"></a>  <span class="at">nrow =</span> <span class="dv">2</span>,</span>
<span id="cb9-7"><a href="machine-learning.html#cb9-7" tabindex="-1"></a>  <span class="at">dimnames =</span> <span class="fu">list</span>(</span>
<span id="cb9-8"><a href="machine-learning.html#cb9-8" tabindex="-1"></a>    <span class="fu">c</span>(<span class="st">&quot;A&quot;</span>, <span class="st">&quot;B&quot;</span>),</span>
<span id="cb9-9"><a href="machine-learning.html#cb9-9" tabindex="-1"></a>    <span class="fu">c</span>(<span class="st">&quot;A&quot;</span>, <span class="st">&quot;B&quot;</span>)</span>
<span id="cb9-10"><a href="machine-learning.html#cb9-10" tabindex="-1"></a>  )</span>
<span id="cb9-11"><a href="machine-learning.html#cb9-11" tabindex="-1"></a>)</span></code></pre></div>
<pre><code>##    A  B
## A 71  1
## B 29 99</code></pre>
<p>Finally let’s plot <code>x1</code> and <code>x2</code> coloured using the latest clustering labelling:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="machine-learning.html#cb11-1" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb11-2"><a href="machine-learning.html#cb11-2" tabindex="-1"></a></span>
<span id="cb11-3"><a href="machine-learning.html#cb11-3" tabindex="-1"></a>portfolio <span class="sc">%&gt;%</span> </span>
<span id="cb11-4"><a href="machine-learning.html#cb11-4" tabindex="-1"></a><span class="fu">ggplot</span>() <span class="sc">+</span></span>
<span id="cb11-5"><a href="machine-learning.html#cb11-5" tabindex="-1"></a>  <span class="fu">geom_point</span>(</span>
<span id="cb11-6"><a href="machine-learning.html#cb11-6" tabindex="-1"></a>    <span class="fu">aes</span>(x1, x2, <span class="at">colour =</span> group_label_stage2)</span>
<span id="cb11-7"><a href="machine-learning.html#cb11-7" tabindex="-1"></a>  )</span></code></pre></div>
<p><img src="book_files/figure-html/15-machine-learning-07-1.png" width="672" /></p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="mortality-projection.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": true,
    "facebook": false,
    "twitter": true,
    "linkedin": true,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["twitter", "linkedin"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": "https://github.com/priyam0k/CS2/edit/main/15-machine-learning.Rmd",
    "text": "Edit"
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": null,
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "section",
    "scroll_highlight": true
  },
  "toolbar": {
    "position": "fixed"
  },
  "info": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
